{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, when, length, size, split, udf, rand, size, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|        id|                date|   query|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|  null|        id|                date|   query|           user|                text|\n",
      "|     4|1999177446|Mon Jun 01 19:46:...|NO_QUERY|       firewalk|oooo... KJ Parker...|\n",
      "|     4|1999177528|Mon Jun 01 19:46:...|NO_QUERY|       TimB5150|@Ericatwitts  Ur ...|\n",
      "|     4|1999177648|Mon Jun 01 19:46:...|NO_QUERY|       JesicaXD|I think im havin ...|\n",
      "|     4|1999177836|Mon Jun 01 19:46:...|NO_QUERY|  MaggieMcNulty|Just made a yummy...|\n",
      "|     4|1999177901|Mon Jun 01 19:46:...|NO_QUERY|       Exklusiv|@alice_verney no ...|\n",
      "|     4|1999178123|Mon Jun 01 19:46:...|NO_QUERY|  BEAUTIFULL308|@UniqueZayas Oooh...|\n",
      "|     4|1999178168|Mon Jun 01 19:46:...|NO_QUERY|   SaturnMoonie|Night my twits. L...|\n",
      "|     4|1999178183|Mon Jun 01 19:46:...|NO_QUERY|     mamaShayee|@kLANing haha tha...|\n",
      "|     4|1999178186|Mon Jun 01 19:46:...|NO_QUERY|       juliadee|@kreksss waittt h...|\n",
      "|     4|1999178215|Mon Jun 01 19:46:...|NO_QUERY|Jessilovesmetal|@adamdailycom oh ...|\n",
      "|     4|1999178267|Mon Jun 01 19:46:...|NO_QUERY|       nchokkan|@msathia ம�?ஹூம�?...|\n",
      "|     4|1999178487|Mon Jun 01 19:46:...|NO_QUERY|         EmbryC|@chelcimac Hey, c...|\n",
      "|     4|1999178529|Mon Jun 01 19:46:...|NO_QUERY|     inflowenza|i keep comin to t...|\n",
      "|     4|1999178976|Mon Jun 01 19:46:...|NO_QUERY| digitalArtform|@AmeliaG somethin...|\n",
      "|     4|1999179021|Mon Jun 01 19:46:...|NO_QUERY|      khokanson|@dlaufenberg @jon...|\n",
      "|     4|1999179032|Mon Jun 01 19:46:...|NO_QUERY|   nathanrdotca| @conradd Good job. |\n",
      "|     4|1999179266|Mon Jun 01 19:46:...|NO_QUERY| Tastelikecrazy|@looneytunes You ...|\n",
      "|     4|1999179359|Mon Jun 01 19:46:...|NO_QUERY|        sophy23|Long work out today |\n",
      "|     4|1999179375|Mon Jun 01 19:46:...|NO_QUERY|serenity_neeley|... Sigh. . . I h...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = spark.read.csv(\"../../data/kaggle/sentiment\", schema=schema, header=False)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to include all three dataframes in the training stage, the data is randomly mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.orderBy(rand(seed=43))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is divided in train and test. Being the train the 80% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "split_index = int(data.count() * 0.8)\n",
    "\n",
    "df_train = data.limit(split_index)\n",
    "df_test = data.subtract(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data analysis has been completed in Jupyter, the text cleaning stage is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets require lots of cleaning but it is inefficient to clean every single tweet because that would consume too much time. A general approach must be implemented for cleaning.\n",
    "\n",
    "* The most common type of words that require cleaning in oov have punctuations at the start or end. Those words doesn't have embeddings because of the trailing punctuations. Punctuations #, @, !, ?, +, &, -, $, =, <, >, |, {, }, ^, ', (, ),[, ], *, %, ..., ', ., :, ; are separated from words\n",
    "* Special characters that are attached to words are removed completely\n",
    "* Contractions are expanded\n",
    "* Urls are removed\n",
    "* Character entity references are replaced with their actual symbols\n",
    "* Typos and slang are corrected, and informal abbreviations are written in their long forms\n",
    "* Some words are replaced with their acronyms and some words are grouped into one\n",
    "* Finally, hashtags and usernames contain lots of information about the context but they are written without spaces in between words so they don't have embeddings. Informational usernames and hashtags should be expanded but there are too many of them. Due to the project deadline, hashtags and usernames haven't been expanded in detail, a list of expanded usernames was taken in order to achive this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|\n",
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "|     0|2264093836|Sun Jun 21 02:45:...|NO_QUERY|  darcyemily| `  then they han...|\n",
      "|     0|1550967451|Sat Apr 18 07:46:...|NO_QUERY|serendipitie|Cannot remember t...|\n",
      "|     4|1827780885|Sun May 17 11:26:...|NO_QUERY|  aliceokoye|watching televisi...|\n",
      "|     0|2190184804|Tue Jun 16 02:00:...|NO_QUERY|   Rubybelle|Book club was goo...|\n",
      "|     4|1982804907|Sun May 31 11:56:...|NO_QUERY|      bwg_uk|On the choo choo ...|\n",
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|\n",
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "|     0|2048943655|Fri Jun 05 16:01:...|NO_QUERY|         rem| .  @ kswedberg  ...|\n",
      "|     0|2261266560|Sat Jun 20 20:31:...|NO_QUERY|chelsiegreen|Mom forcing me to...|\n",
      "|     0|2283032051|Mon Jun 22 12:06:...|NO_QUERY|  AlyxxDione| @ SexXyBlackines...|\n",
      "|     4|1793395522|Thu May 14 02:31:...|NO_QUERY|    wendy_uk| @ aliflyby Hope ...|\n",
      "|     4|2053133671|Sat Jun 06 03:24:...|NO_QUERY|SayuriYubari|I´m doing a cake !  |\n",
      "+------+----------+--------------------+--------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "replace_text = udf(lambda text: \n",
    "                    text.replace(\"å_\", \"\")\n",
    "                        .replace(\"fromåÊwounds\", \"from wounds\")\n",
    "                        .replace(\"åÊ\", \"\")\n",
    "                        .replace(\"åÈ\", \"\")\n",
    "                        .replace(\"JapÌ_n\", \"Japan\")\n",
    "                        .replace(\"Ì©\", \"e\")\n",
    "                        .replace(\"å¨\", \"\")\n",
    "                        .replace(\"SuruÌ¤\", \"Suruc\")\n",
    "                        .replace(\"åÇ\", \"\")\n",
    "                        .replace(\"å£3million\", \"3 million\")\n",
    "                        .replace(\"åÀ\", \"\")\n",
    "                        .replace(\"he's\", \"he is\")\n",
    "                        .replace(\"there's\", \"there is\")\n",
    "                        .replace(\"We're\", \"We are\")\n",
    "                        .replace(\"That's\", \"That is\")\n",
    "                        .replace(\"won't\", \"will not\")\n",
    "                        .replace(\"they're\", \"they are\")\n",
    "                        .replace(\"Can't\", \"Cannot\")\n",
    "                        .replace(\"wasn't\", \"was not\")\n",
    "                        .replace(\"aren't\", \"are not\")\n",
    "                        .replace(\"isn't\", \"is not\")\n",
    "                        .replace(\"What's\", \"What is\")\n",
    "                        .replace(\"haven't\", \"have not\")\n",
    "                        .replace(\"hasn't\", \"has not\")\n",
    "                        .replace(\"There's\", \"There is\")\n",
    "                        .replace(\"He's\", \"He is\")\n",
    "                        .replace(\"It's\", \"It is\")\n",
    "                        .replace(\"You're\", \"You are\")\n",
    "                        .replace(\"I'M\", \"I am\")\n",
    "                        .replace(\"shouldn't\", \"should not\")\n",
    "                        .replace(\"wouldn't\", \"would not\")\n",
    "                        .replace(\"i'm\", \"I am\")\n",
    "                        .replace(\"I'm\", \"I am\")\n",
    "                        .replace(\"Isn't\", \"is not\")\n",
    "                        .replace(\"Here's\", \"Here is\")\n",
    "                        .replace(\"you've\", \"you have\")\n",
    "                        .replace(\"we're\", \"we are\")\n",
    "                        .replace(\"what's\", \"what is\")\n",
    "                        .replace(\"couldn't\", \"could not\")\n",
    "                        .replace(\"we've\", \"we have\")\n",
    "                        .replace(\"who's\", \"who is\")\n",
    "                        .replace(\"y'all\", \"you all\")\n",
    "                        .replace(\"would've\", \"would have\")\n",
    "                        .replace(\"it'll\", \"it will\")\n",
    "                        .replace(\"we'll\", \"we will\")\n",
    "                        .replace(\"We've\", \"We have\")\n",
    "                        .replace(\"he'll\", \"he will\")\n",
    "                        .replace(\"Y'all\", \"You all\")\n",
    "                        .replace(\"Weren't\", \"Were not\")\n",
    "                        .replace(\"Didn't\", \"Did not\")\n",
    "                        .replace(\"they'll\", \"they will\")\n",
    "                        .replace(\"they'd\", \"they would\")\n",
    "                        .replace(\"DON'T\", \"DO NOT\")\n",
    "                        .replace(\"they've\", \"they have\")\n",
    "                        .replace(\"i'd\", \"I would\")\n",
    "                        .replace(\"should've\", \"should have\")\n",
    "                        .replace(\"where's\", \"where is\")\n",
    "                        .replace(\"we'd\", \"we would\")\n",
    "                        .replace(\"i'll\", \"I will\")\n",
    "                        .replace(\"weren't\", \"were not\")\n",
    "                        .replace(\"They're\", \"They are\")\n",
    "                        .replace(\"let's\", \"let us\")\n",
    "                        .replace(\"it's\", \"it is\")\n",
    "                        .replace(\"can't\", \"cannot\")\n",
    "                        .replace(\"don't\", \"do not\")\n",
    "                        .replace(\"you're\", \"you are\")\n",
    "                        .replace(\"i've\", \"I have\")\n",
    "                        .replace(\"that's\", \"that is\")\n",
    "                        .replace(\"i'll\", \"I will\")\n",
    "                        .replace(\"doesn't\", \"does not\")\n",
    "                        .replace(\"i'd\", \"I would\")\n",
    "                        .replace(\"didn't\", \"did not\")\n",
    "                        .replace(\"ain't\", \"am not\")\n",
    "                        .replace(\"you'll\", \"you will\")\n",
    "                        .replace(\"I've\", \"I have\")\n",
    "                        .replace(\"Don't\", \"do not\")\n",
    "                        .replace(\"I'll\", \"I will\")\n",
    "                        .replace(\"I'd\", \"I would\")\n",
    "                        .replace(\"Let's\", \"Let us\")\n",
    "                        .replace(\"you'd\", \"You would\")\n",
    "                        .replace(\"It's\", \"It is\")\n",
    "                        .replace(\"Ain't\", \"am not\")\n",
    "                        .replace(\"Haven't\", \"Have not\")\n",
    "                        .replace(\"Could've\", \"Could have\")\n",
    "                        .replace(\"youve\", \"you have\")\n",
    "                        .replace(\"donå«t\", \"do not\")\n",
    "                        .replace(\"@\", \" @ \")\n",
    "                        .replace(\"#\", \" # \")\n",
    "                        .replace(\"!\", \" ! \")\n",
    "                        .replace(\"?\", \" ? \")\n",
    "                        .replace(\"+\", \" + \")\n",
    "                        .replace(\"&\", \" & \")\n",
    "                        .replace(\"*\", \" * \")\n",
    "                        .replace(\"[\", \" [ \")\n",
    "                        .replace(\"]\", \" ] \")\n",
    "                        .replace(\"-\", \" - \")\n",
    "                        .replace(\"%\", \" % \")\n",
    "                        .replace(\".\", \" . \")\n",
    "                        .replace(\":\", \" : \")\n",
    "                        .replace(\"/\", \" / \")\n",
    "                        .replace(\"(\", \" ( \")\n",
    "                        .replace(\")\", \" ) \")\n",
    "                        .replace(\";\", \" ; \")\n",
    "                        .replace(\"$\", \" $ \")\n",
    "                        .replace(\"=\", \" = \")\n",
    "                        .replace(\">\", \" > \")\n",
    "                        .replace(\"<\", \" < \")\n",
    "                        .replace(\"|\", \" | \")\n",
    "                        .replace(\"{\", \" { \")\n",
    "                        .replace(\"}\", \" } \")\n",
    "                        .replace(\"^\", \" ^ \")\n",
    "                        .replace(\"'\", \" ' \")\n",
    "                        .replace(\"`\", \" ` \")\n",
    "                        .replace(\"...\", \" ... \")\n",
    "                        .replace(\"..\", \" ... \") if text is not None else None, StringType())\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_train.withColumn(\"text\", replace_text(df_train[\"text\"]))\n",
    "df_test = df_test.withColumn(\"text\", replace_text(df_test[\"text\"]))\n",
    "\n",
    "# Mostrar el DataFrame transformado\n",
    "df_train.show(5)\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tokenizer:** <br>\n",
    "The Tokenizer is a feature transformer that takes an input text column and splits it into individual words or tokens. It is used to preprocess the text data before applying any machine learning algorithms. In this case, the input text column is \"text_cleaned\" which contains the preprocessed and cleaned text data. The Tokenizer transforms the \"text_cleaned\" column into a new column called \"tokens\" where each row contains an array of tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|               words|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "|     0|2264093836|Sun Jun 21 02:45:...|NO_QUERY|  darcyemily| `  then they han...|[then, they, hand...|\n",
      "|     0|1550967451|Sat Apr 18 07:46:...|NO_QUERY|serendipitie|Cannot remember t...|[cannot, remember...|\n",
      "|     4|1827780885|Sun May 17 11:26:...|NO_QUERY|  aliceokoye|watching televisi...|[watching, televi...|\n",
      "|     0|2190184804|Tue Jun 16 02:00:...|NO_QUERY|   Rubybelle|Book club was goo...|[book, club, was,...|\n",
      "|     4|1982804907|Sun May 31 11:56:...|NO_QUERY|      bwg_uk|On the choo choo ...|[on, the, choo, c...|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|               words|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "|     0|2048943655|Fri Jun 05 16:01:...|NO_QUERY|         rem| .  @ kswedberg  ...|[kswedberg, amp, ...|\n",
      "|     0|2261266560|Sat Jun 20 20:31:...|NO_QUERY|chelsiegreen|Mom forcing me to...|[mom, forcing, me...|\n",
      "|     0|2283032051|Mon Jun 22 12:06:...|NO_QUERY|  AlyxxDione| @ SexXyBlackines...|[sexxyblackinese,...|\n",
      "|     4|1793395522|Thu May 14 02:31:...|NO_QUERY|    wendy_uk| @ aliflyby Hope ...|[aliflyby, hope, ...|\n",
      "|     4|2053133671|Sat Jun 06 03:24:...|NO_QUERY|SayuriYubari|I´m doing a cake !  |[i, m, doing, a, ...|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words_train = regex_tokenizer.transform(df_train)\n",
    "raw_words_test = regex_tokenizer.transform(df_test)\n",
    "raw_words_train.show(5)\n",
    "raw_words_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **StopWords:** <br>\n",
    "Stop words are commonly used words in a language that typically do not carry much meaning or contribute significantly to the overall understanding of a text. Examples of stop words in English include \"the\", \"is\", \"and\", \"a\", and \"an\". These words are often filtered out or removed from text data during natural language processing tasks, such as text classification or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:21:06 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 62 (TID 193): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|               words|            filtered|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "|     0|2264093836|Sun Jun 21 02:45:...|NO_QUERY|  darcyemily| `  then they han...|[then, they, hand...|[handed, folded, ...|\n",
      "|     0|1550967451|Sat Apr 18 07:46:...|NO_QUERY|serendipitie|Cannot remember t...|[cannot, remember...|[remember, websit...|\n",
      "|     4|1827780885|Sun May 17 11:26:...|NO_QUERY|  aliceokoye|watching televisi...|[watching, televi...|[watching, televi...|\n",
      "|     0|2190184804|Tue Jun 16 02:00:...|NO_QUERY|   Rubybelle|Book club was goo...|[book, club, was,...|[book, club, good...|\n",
      "|     4|1982804907|Sun May 31 11:56:...|NO_QUERY|      bwg_uk|On the choo choo ...|[on, the, choo, c...|[choo, choo, london]|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|        user|                text|               words|            filtered|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "|     0|2048943655|Fri Jun 05 16:01:...|NO_QUERY|         rem| .  @ kswedberg  ...|[kswedberg, amp, ...|[kswedberg, amp, ...|\n",
      "|     0|2261266560|Sat Jun 20 20:31:...|NO_QUERY|chelsiegreen|Mom forcing me to...|[mom, forcing, me...|[mom, forcing, ea...|\n",
      "|     0|2283032051|Mon Jun 22 12:06:...|NO_QUERY|  AlyxxDione| @ SexXyBlackines...|[sexxyblackinese,...|[sexxyblackinese,...|\n",
      "|     4|1793395522|Thu May 14 02:31:...|NO_QUERY|    wendy_uk| @ aliflyby Hope ...|[aliflyby, hope, ...|[aliflyby, hope, ...|\n",
      "|     4|2053133671|Sat Jun 06 03:24:...|NO_QUERY|SayuriYubari|I´m doing a cake !  |[i, m, doing, a, ...|           [m, cake]|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "words_df_train = remover.transform(raw_words_train)\n",
    "words_df_test = remover.transform(raw_words_test)\n",
    "words_df_train.show(5)\n",
    "words_df_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CountVectorizer:** <br>\n",
    "The CountVectorizer is a feature transformer that converts a collection of text documents into a matrix of token counts. It takes an input column of tokens and outputs a sparse vector representation of the token counts. In this case, the input column is \"tokens\" which contains the array of tokens generated by the Tokenizer. The CountVectorizer learns a vocabulary of distinct tokens from the training data and represents each document as a vector of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:21:46 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|target|        id|                date|   query|        user|                text|               words|            filtered|            features|label|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     0|2264093836|Sun Jun 21 02:45:...|NO_QUERY|  darcyemily| `  then they han...|[then, they, hand...|[handed, folded, ...|(262144,[34,153,9...|    0|\n",
      "|     0|1550967451|Sat Apr 18 07:46:...|NO_QUERY|serendipitie|Cannot remember t...|[cannot, remember...|[remember, websit...|(262144,[386,725,...|    0|\n",
      "|     4|1827780885|Sun May 17 11:26:...|NO_QUERY|  aliceokoye|watching televisi...|[watching, televi...|[watching, televi...|(262144,[13,63,24...|    4|\n",
      "|     0|2190184804|Tue Jun 16 02:00:...|NO_QUERY|   Rubybelle|Book club was goo...|[book, club, was,...|[book, club, good...|(262144,[0,59,74,...|    0|\n",
      "|     4|1982804907|Sun May 31 11:56:...|NO_QUERY|      bwg_uk|On the choo choo ...|[on, the, choo, c...|[choo, choo, london]|(262144,[613,1372...|    4|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:22:21 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|target|        id|                date|   query|        user|                text|               words|            filtered|            features|label|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     0|2048943655|Fri Jun 05 16:01:...|NO_QUERY|         rem| .  @ kswedberg  ...|[kswedberg, amp, ...|[kswedberg, amp, ...|(223420,[21,503,7...|    0|\n",
      "|     0|2261266560|Sat Jun 20 20:31:...|NO_QUERY|chelsiegreen|Mom forcing me to...|[mom, forcing, me...|[mom, forcing, ea...|(223420,[24,44,59...|    0|\n",
      "|     0|2283032051|Mon Jun 22 12:06:...|NO_QUERY|  AlyxxDione| @ SexXyBlackines...|[sexxyblackinese,...|[sexxyblackinese,...|(223420,[13,33,28...|    0|\n",
      "|     4|1793395522|Thu May 14 02:31:...|NO_QUERY|    wendy_uk| @ aliflyby Hope ...|[aliflyby, hope, ...|[aliflyby, hope, ...|(223420,[39,42,47...|    4|\n",
      "|     4|2053133671|Sat Jun 06 03:24:...|NO_QUERY|SayuriYubari|I´m doing a cake !  |[i, m, doing, a, ...|           [m, cake]|(223420,[294,749]...|    4|\n",
      "+------+----------+--------------------+--------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "\n",
    "# train\n",
    "model_train = cv.fit(words_df_train)\n",
    "countVectorizer_train = model_train.transform(words_df_train)\n",
    "countVectorizer_train = countVectorizer_train.withColumn(\"label\",col('target'))\n",
    "countVectorizer_train.show(5)\n",
    "\n",
    "# test\n",
    "model_test = cv.fit(words_df_test)\n",
    "countVectorizer_test = model_test.transform(words_df_test)\n",
    "countVectorizer_test= countVectorizer_test.withColumn(\"label\",col('target'))\n",
    "countVectorizer_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "As evaluation metrics ROC and Accuracy will be used. <br>\n",
    "\n",
    "* **ROC curve** is a graphical representation that shows the performance of a binary classification model as the decision threshold is varied. On the x-axis, the false positive rate (FPR) is plotted, which is the proportion of negative instances incorrectly classified as positive. On the y-axis, the true positive rate (TPR) is plotted, which is the proportion of positive instances correctly classified as positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate train and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, validate) = countVectorizer_train.randomSplit([0.8, 0.2],seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = countVectorizer_train\n",
    "testData = countVectorizer_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:22:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:22:44 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:22:59 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"features\")\n",
    "nbModel = nb.fit(train)\n",
    "nb_predictions = nbModel.transform(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:23:13 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:23:25 WARN DAGScheduler: Broadcasting large task binary with size 7.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.5084758366680321\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:23:40 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:23:57 WARN DAGScheduler: Broadcasting large task binary with size 7.0 MiB\n",
      "24/04/01 21:24:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes is = 0.393824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 21:24:09 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:24:26 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:24:38 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:24:48 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:25:01 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:25:07 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 21:25:15 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "24/04/01 21:27:00 WARN DAGScheduler: Broadcasting large task binary with size 1033.7 KiB\n",
      "24/04/01 21:27:01 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "24/04/01 21:27:07 WARN MemoryStore: Not enough space to cache rdd_412_0 in memory! (computed 113.0 MiB so far)\n",
      "24/04/01 21:27:07 WARN BlockManager: Persisting block rdd_412_0 to disk instead.\n",
      "24/04/01 22:06:12 WARN MemoryStore: Not enough space to cache rdd_412_0 in memory! (computed 272.8 MiB so far)\n",
      "24/04/01 22:29:49 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "24/04/01 22:29:50 WARN MemoryStore: Not enough space to cache rdd_412_0 in memory! (computed 272.8 MiB so far)\n",
      "24/04/01 22:53:23 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "24/04/01 22:53:24 WARN MemoryStore: Not enough space to cache rdd_412_0 in memory! (computed 272.8 MiB so far)\n",
      "24/04/01 23:17:06 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 23:17:20 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "[Stage 234:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+----------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|target|        id|                date|   query|      user|                text|               words|            filtered|            features|label|       rawPrediction|         probability|prediction|\n",
      "+------+----------+--------------------+--------+----------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|  mattycus| @ Kenichan I div...|[kenichan, i, div...|[kenichan, dived,...|(262144,[4,218,25...|    0|[472520.0,0.0,0.0...|[0.49273083332116...|       4.0|\n",
      "|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|   mybirch|         Need a hug |      [need, a, hug]|         [need, hug]|(262144,[35,810],...|    0|[472520.0,0.0,0.0...|[0.49273083332116...|       4.0|\n",
      "|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|   mimismo| @ twittera que m...|[twittera, que, m...|[twittera, que, m...|(262144,[2386,777...|    0|[472520.0,0.0,0.0...|[0.49273083332116...|       4.0|\n",
      "|     0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|HairByJess| @ iamjazzyfizzle...|[iamjazzyfizzle, ...|[iamjazzyfizzle, ...|(262144,[11,34,47...|    0|[18465.0,0.0,0.0,...|[0.84215087111192...|       0.0|\n",
      "|     0|1467814119|Mon Apr 06 22:20:...|NO_QUERY| cooliodoc| @ angry_barista ...|[angry_barista, i...|[angry_barista, b...|(262144,[743,3181...|    0|[472520.0,0.0,0.0...|[0.49273083332116...|       4.0|\n",
      "+------+----------+--------------------+--------+----------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'target', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "dtPreds = dtModel.transform(validate)\n",
    "dtPreds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 23:17:30 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/04/01 23:17:42 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "[Stage 241:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Trees is = 0.52991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator.evaluate(dtPreds)\n",
    "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "gbtPreds = gbtModel.transform(validate)\n",
    "gbtPreds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.7921680466831243\n"
     ]
    }
   ],
   "source": [
    "gbtEval = BinaryClassificationEvaluator()\n",
    "gbtROC = gbtEval.evaluate(gbtPreds, {gbtEval.metricName: \"areaUnderROC\"})\n",
    "print(\"Test Area Under ROC: \" + str(gbtROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 487:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBT is = 0.759571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "gb_accuracy = evaluator.evaluate(gbtPreds)\n",
    "print(\"Accuracy of GBT is = %g\"% (gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBTClassifier is used to make predictions because it has the best evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 649:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(trainData)\n",
    "gbtPreds = gbtModel.transform(testData)\n",
    "predictions = gbtPreds.select('id','prediction')\n",
    "predictions.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ready\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "stopword=set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,SpatialDropout1D\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_paths = [\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part1',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part2',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part3',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part4',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part5',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part6',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part7',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part8',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part9',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part10',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part11',\n",
    "    '../data/0 train/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part12',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(part_path, encoding='utf-8', lineterminator='\\n', on_bad_lines='skip') for part_path in part_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'text\\r': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire \\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date     query             user                                               text\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF  my whole body feels itchy and like its on fire \\r\n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all....\n",
       "...         ...         ...                           ...       ...              ...                                                ...\n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY  AmandaMarie1028  Just woke up. Having no school is the best fee...\n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY           bpbabe  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean(text)` function takes a text as input and performs a series of transformations to clean and normalize the text. Here is a brief explanation of what each step does:\n",
    "\n",
    "* Converts the text to lowercase.\n",
    "* Removes special characters and replaces specific characters with others.\n",
    "* Replaces common contractions with their full forms.\n",
    "* Replaces character entity references such as \">\" and \"<\".\n",
    "* Corrects typos, slang, and informal abbreviations.\n",
    "* Replaces hashtags and social media usernames with their full forms or descriptions.\n",
    "* Cleans and normalizes words related to natural disasters, current events, and other specific words.\n",
    "* Performs additional corrections and normalizations in the text.\n",
    "* Removes URLs starting with \"http://\" or \"https://\" followed by alphanumeric characters.\n",
    "* Replaces each punctuation and special character with a space and the punctuation/special character itself.\n",
    "* Replaces specific acronyms with their expanded forms or related terms.\n",
    "* Removes Tags, Links, and Punctuation.\n",
    "* Does Stopword Removal and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text): \n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "    text = re.sub(r\"donå«t\", \"do not\", text)   \n",
    "            \n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", text)\n",
    "    text = re.sub(r\"recentlu\", \"recently\", text)\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n",
    "    text = re.sub(r\"amirite\", \"am I right\", text)\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", text)\n",
    "    text = re.sub(r\"8/5/2015\", \"2015-08-05\", text)\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n",
    "    text = re.sub(r\"8/6/2015\", \"2015-08-06\", text)\n",
    "    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n",
    "    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)   \n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n",
    "    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n",
    "    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n",
    "    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n",
    "    text = re.sub(r\"MTVHottest\", \"MTV Hottest\", text)\n",
    "    text = re.sub(r\"TrapMusic\", \"Trap Music\", text)\n",
    "    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n",
    "    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n",
    "    text = re.sub(r\"socialnews\", \"social news\", text)\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"onlinecommunities\", \"online communities\", text)\n",
    "    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n",
    "    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n",
    "    text = re.sub(r\"Meat-Loving\", \"Meat Loving\", text)\n",
    "    text = re.sub(r\"facialabuse\", \"facial abuse\", text)\n",
    "    text = re.sub(r\"LakeCounty\", \"Lake County\", text)\n",
    "    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n",
    "    text = re.sub(r\"withheavenly\", \"with heavenly\", text)\n",
    "    text = re.sub(r\"thankU\", \"thank you\", text)\n",
    "    text = re.sub(r\"iTunesMusic\", \"iTunes Music\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n",
    "    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n",
    "    text = re.sub(r\"animalrescue\", \"animal rescue\", text)\n",
    "    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text)\n",
    "    text = re.sub(r\"aRmageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n",
    "    text = re.sub(r\"GodsLove\", \"God's Love\", text)\n",
    "    text = re.sub(r\"bookboost\", \"book boost\", text)\n",
    "    text = re.sub(r\"ibooklove\", \"I book love\", text)\n",
    "    text = re.sub(r\"NestleIndia\", \"Nestle India\", text)\n",
    "    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\", text)\n",
    "    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text)\n",
    "    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text)\n",
    "    text = re.sub(r\"weathernetwork\", \"weather network\", text)\n",
    "    text = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", text)\n",
    "    text = re.sub(r\"Hostage&2\", \"Hostage & 2\", text)\n",
    "    text = re.sub(r\"GOPDebate\", \"GOP Debate\", text)\n",
    "    text = re.sub(r\"RickPerry\", \"Rick Perry\", text)\n",
    "    text = re.sub(r\"frontpage\", \"front page\", text)\n",
    "    text = re.sub(r\"NewsIntexts\", \"News In texts\", text)\n",
    "    text = re.sub(r\"ViralSpell\", \"Viral Spell\", text)\n",
    "    text = re.sub(r\"til_now\", \"until now\", text)\n",
    "    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n",
    "    text = re.sub(r\"ZippedNews\", \"Zipped News\", text)\n",
    "    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\", text)\n",
    "    text = re.sub(r\"53inch\", \"53 inch\", text)\n",
    "    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"Beyhive\", \"Beyonce hive\", text)\n",
    "    text = re.sub(r\"IDFire\", \"Idaho Fire\", text)\n",
    "    text = re.sub(r\"DETECTADO\", \"Detected\", text)\n",
    "    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n",
    "    text = re.sub(r\"Listen/Buy\", \"Listen / Buy\", text)\n",
    "    text = re.sub(r\"NickCannon\", \"Nick Cannon\", text)\n",
    "    text = re.sub(r\"FaroeIslands\", \"Faroe Islands\", text)\n",
    "    text = re.sub(r\"yycstorm\", \"Calgary Storm\", text)\n",
    "    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text)\n",
    "    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\", text)\n",
    "    text = re.sub(r\"justinbieber\", \"justin bieber\", text)  \n",
    "    text = re.sub(r\"UTC2015\", \"UTC 2015\", text)\n",
    "    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n",
    "    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n",
    "    text = re.sub(r\"LivingSafely\", \"Living Safely\", text)\n",
    "    text = re.sub(r\"FIFA16\", \"Fifa 2016\", text)\n",
    "    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n",
    "    text = re.sub(r\"bbcnews\", \"bbc news\", text)\n",
    "    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text)\n",
    "    text = re.sub(r\"c4news\", \"c4 news\", text)\n",
    "    text = re.sub(r\"OBLITERATION\", \"obliteration\", text)\n",
    "    text = re.sub(r\"MUDSLIDE\", \"mudslide\", text)\n",
    "    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n",
    "    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"LondonFire\", \"London Fire\", text)\n",
    "    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\", text)\n",
    "    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text)\n",
    "    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\", text)\n",
    "    text = re.sub(r\"LiveOnK2\", \"Live On K2\", text)\n",
    "    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text)\n",
    "    text = re.sub(r\"nikeplus\", \"nike plus\", text)\n",
    "    text = re.sub(r\"david_cameron\", \"David Cameron\", text)\n",
    "    text = re.sub(r\"peterjukes\", \"Peter Jukes\", text)\n",
    "    text = re.sub(r\"JamesMelville\", \"James Melville\", text)\n",
    "    text = re.sub(r\"megynkelly\", \"Megyn Kelly\", text)\n",
    "    text = re.sub(r\"cnewslive\", \"C News Live\", text)\n",
    "    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text)\n",
    "    text = re.sub(r\"textLikeItsSeptember11th2001\", \"text like it is september 11th 2001\", text)\n",
    "    text = re.sub(r\"cbplawyers\", \"cbp lawyers\", text)\n",
    "    text = re.sub(r\"fewmoretexts\", \"few more texts\", text)\n",
    "    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n",
    "    text = re.sub(r\"cjoyner\", \"Chris Joyner\", text)\n",
    "    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n",
    "    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n",
    "    text = re.sub(r\"MikeParrActor\", \"Michael Parr\", text)\n",
    "    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text)\n",
    "    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text)\n",
    "    text = re.sub(r\"realmandyrain\", \"Mandy Rain\", text)\n",
    "    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text)\n",
    "    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\", text)\n",
    "    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n",
    "    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text)\n",
    "    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\", text)\n",
    "    text = re.sub(r\"ShaunKing\", \"Shaun King\", text)\n",
    "    text = re.sub(r\"MeekMill\", \"Meek Mill\", text)\n",
    "    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n",
    "    text = re.sub(r\"GRupdates\", \"GR updates\", text)\n",
    "    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n",
    "    text = re.sub(r\"braininjury\", \"brain injury\", text)\n",
    "    text = re.sub(r\"auspol\", \"Australian politics\", text)\n",
    "    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text)\n",
    "    text = re.sub(r\"calgaryweather\", \"Calgary Weather\", text)\n",
    "    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text)\n",
    "    text = re.sub(r\"edsheeran\", \"Ed Sheeran\", text)\n",
    "    text = re.sub(r\"TrueHeroes\", \"True Heroes\", text)\n",
    "    text = re.sub(r\"S3XLEAK\", \"sex leak\", text)\n",
    "    text = re.sub(r\"ComplexMag\", \"Complex Magazine\", text)\n",
    "    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text)\n",
    "    text = re.sub(r\"CityofCalgary\", \"City of Calgary\", text)\n",
    "    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text)\n",
    "    text = re.sub(r\"SummerFate\", \"Summer Fate\", text)\n",
    "    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n",
    "    text = re.sub(r\"offers2go\", \"offers to go\", text)\n",
    "    text = re.sub(r\"foodscare\", \"food scare\", text)\n",
    "    text = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", text)\n",
    "    text = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", text)\n",
    "    text = re.sub(r\"GamerGate\", \"Gamer Gate\", text)\n",
    "    text = re.sub(r\"IHHen\", \"Humanitarian Relief\", text)\n",
    "    text = re.sub(r\"spinningbot\", \"spinning bot\", text)\n",
    "    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\", text)\n",
    "    text = re.sub(r\"TAXIWAYS\", \"taxi ways\", text)\n",
    "    text = re.sub(r\"Calum5SOS\", \"Calum Hood\", text)\n",
    "    text = re.sub(r\"po_st\", \"po.st\", text)\n",
    "    text = re.sub(r\"scoopit\", \"scoop.it\", text)\n",
    "    text = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", text)\n",
    "    text = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", text)\n",
    "    text = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", text)\n",
    "    text = re.sub(r\"rapidcity\", \"Rapid City\", text)\n",
    "    text = re.sub(r\"OutBid\", \"outbid\", text)\n",
    "    text = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", text)\n",
    "    text = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", text)\n",
    "    text = re.sub(r\"15PM\", \"15 PM\", text)\n",
    "    text = re.sub(r\"OriginalFunko\", \"Funko\", text)\n",
    "    text = re.sub(r\"rightwaystan\", \"Richard Tan\", text)\n",
    "    text = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", text)\n",
    "    text = re.sub(r\"RT_America\", \"RT America\", text)\n",
    "    text = re.sub(r\"narendramodi\", \"Narendra Modi\", text)\n",
    "    text = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", text)\n",
    "    text = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", text)\n",
    "    text = re.sub(r\"alexbelloli\", \"Alex Belloli\", text)\n",
    "    text = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", text)\n",
    "    text = re.sub(r\"gunsense\", \"gun sense\", text)\n",
    "    text = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", text)\n",
    "    text = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", text)\n",
    "    text = re.sub(r\"samanthaturne19\", \"Samantha Turner\", text)\n",
    "    text = re.sub(r\"JonVoyage\", \"Jon Stewart\", text)\n",
    "    text = re.sub(r\"renew911health\", \"renew 911 health\", text)\n",
    "    text = re.sub(r\"SuryaRay\", \"Surya Ray\", text)\n",
    "    text = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", text)\n",
    "    text = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", text)\n",
    "    text = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", text)\n",
    "    text = re.sub(r\"pmarca\", \"Marc Andreessen\", text)\n",
    "    text = re.sub(r\"pdx911\", \"Portland Police\", text)\n",
    "    text = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", text)\n",
    "    text = re.sub(r\"Japton\", \"Arkansas\", text)\n",
    "    text = re.sub(r\"RouteComplex\", \"Route Complex\", text)\n",
    "    text = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", text)\n",
    "    text = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", text)\n",
    "    text = re.sub(r\"Politifiact\", \"PolitiFact\", text)\n",
    "    text = re.sub(r\"Hiroshima70\", \"Hiroshima\", text)\n",
    "    text = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", text)\n",
    "    text = re.sub(r\"versethe\", \"verse the\", text)\n",
    "    text = re.sub(r\"TubeStrike\", \"Tube Strike\", text)\n",
    "    text = re.sub(r\"MissionHills\", \"Mission Hills\", text)\n",
    "    text = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", text)\n",
    "    text = re.sub(r\"NANKANA\", \"Nankana\", text)\n",
    "    text = re.sub(r\"SAHIB\", \"Sahib\", text)\n",
    "    text = re.sub(r\"PAKPATTAN\", \"Pakpattan\", text)\n",
    "    text = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", text)\n",
    "    text = re.sub(r\"gofundme\", \"go fund me\", text)\n",
    "    text = re.sub(r\"pmharper\", \"Stephen Harper\", text)\n",
    "    text = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", text)\n",
    "    text = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", text)\n",
    "    text = re.sub(r\"bancodeseries\", \"banco de series\", text)\n",
    "    text = re.sub(r\"timkaine\", \"Tim Kaine\", text)\n",
    "    text = re.sub(r\"IdentityTheft\", \"Identity Theft\", text)\n",
    "    text = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", text)\n",
    "    text = re.sub(r\"mishacollins\", \"Misha Collins\", text)\n",
    "    text = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", text)\n",
    "    text = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", text)\n",
    "    text = re.sub(r\"Kowing\", \"Knowing\", text)\n",
    "    text = re.sub(r\"ScreamQueens\", \"Scream Queens\", text)\n",
    "    text = re.sub(r\"AskCharley\", \"Ask Charley\", text)\n",
    "    \n",
    "    # Urls\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n",
    "    text = re.sub(r\"mÌ¼sica\", \"music\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)    \n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n",
    "    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)   \n",
    "    \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 13s\n",
      "Wall time: 10min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['text'] = data['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigns the 'text' column to the variable X_data and the 'target' column to the variable y_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['target'] == 4, 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            switchfoot http      twitpic  com      awww ...\n",
       "1          upset cannot updat facebook text       might c...\n",
       "2            kenichan dive mani time ball   manag save   ...\n",
       "3                         whole bodi feel itchi like fire \\r\n",
       "4            nationwideclass behav   i mad     cannot see...\n",
       "                                 ...                        \n",
       "1599995                      woke   school best feel ever \\r\n",
       "1599996    thewdb  com    cool hear old walt interview   ...\n",
       "1599997                    readi mojo makeov   ask detail \\r\n",
       "1599998    happi  birthday boo alll time       tupac amar...\n",
       "1599999    happi   charitytuesday   thenspcc   sparkschar...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = data['text']\n",
    "y_data = data['target']\n",
    "\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_truncated, X_discarded, y_truncated, y_discarded = train_test_split(X_data, y_data, train_size=0.008, stratify=y_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000 1200000\n",
      "400000 400000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600 9600\n",
      "3200 3200\n"
     ]
    }
   ],
   "source": [
    "x_train_trunc, x_test_trunc, y_train_trunc, y_test_trunc = train_test_split(X_truncated, y_truncated, random_state=42)\n",
    "print(len(x_train_trunc), len(y_train_trunc))\n",
    "print(len(x_test_trunc), len(y_test_trunc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer and Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer** operates on each individual text and performs the following steps:\n",
    "\n",
    "* **Tokenization**: It splits each document into individual words or terms, which are also referred to as tokens. \n",
    "* **Counting**: It counts the occurrence of each token in each document and creates a matrix where columns represent unique tokens.\n",
    "* **Vectorization**: It assigns a numerical value (count) to each token in each document, indicating how many times the token appears in that document.\n",
    "* **Vocabulary** **Creation**: It builds a vocabulary of unique tokens based on the training data. Each token corresponds to a specific column in the matrix.\n",
    "* **Transforming** **Test** **Data**: When applied to test data, the CountVectorizer uses the learned vocabulary from the training data and creates the matrix of token counts using the same columns as in the training matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** are a set of commonly used words in a language that are considered insignificant and are often removed during text preprocessing or natural language processing tasks. These words are filtered out because they typically do not carry much meaning or contribute significantly to the overall understanding of the text. Examples of stop words in English include \"the,\" \"is,\" \"and,\" \"a,\" \"an,\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "x_train_vectorizer=count.fit_transform(x_train)\n",
    "x_test_vectorizer=count.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nathyeah': 11299575,\n",
       " 'dame': 3510614,\n",
       " 'nathyeah dame': 11299576,\n",
       " 'boburnham': 1897620,\n",
       " 'thesaurus': 16002931,\n",
       " 'world': 18390753,\n",
       " 'sale': 13934745,\n",
       " 'ended': 4717110,\n",
       " 'boburnham thesaurus': 1897646,\n",
       " 'thesaurus world': 16002949,\n",
       " 'world sale': 18399450,\n",
       " 'sale ended': 13935346,\n",
       " 'boburnham thesaurus world': 1897647,\n",
       " 'thesaurus world sale': 16002950,\n",
       " 'world sale ended': 18399451,\n",
       " 'boburnham thesaurus world sale': 1897648,\n",
       " 'thesaurus world sale ended': 16002951,\n",
       " 'boburnham thesaurus world sale ended': 1897649,\n",
       " 'jojoalexander': 8389556,\n",
       " 'ight': 7826904,\n",
       " 'let': 9295251,\n",
       " 'lil': 9501803,\n",
       " 'white': 18031158,\n",
       " 'boy': 2016200,\n",
       " 'know': 8927753,\n",
       " 'hahaha': 6769374,\n",
       " 'jojoalexander ight': 8389557,\n",
       " 'ight let': 7826957,\n",
       " 'let lil': 9306409,\n",
       " 'lil white': 9507592,\n",
       " 'white boy': 18031605,\n",
       " 'boy know': 2019807,\n",
       " 'know hahaha': 8951999,\n",
       " 'jojoalexander ight let': 8389558,\n",
       " 'ight let lil': 7826958,\n",
       " 'let lil white': 9306416,\n",
       " 'lil white boy': 9507593,\n",
       " 'white boy know': 18031615,\n",
       " 'boy know hahaha': 2019811,\n",
       " 'jojoalexander ight let lil': 8389559,\n",
       " 'ight let lil white': 7826959,\n",
       " 'let lil white boy': 9306417,\n",
       " 'lil white boy know': 9507594,\n",
       " 'white boy know hahaha': 18031616,\n",
       " 'jojoalexander ight let lil white': 8389560,\n",
       " 'ight let lil white boy': 7826960,\n",
       " 'let lil white boy know': 9306418,\n",
       " 'lil white boy know hahaha': 9507595,\n",
       " 'tweetlater': 16920561,\n",
       " 'pro': 12871399,\n",
       " 'way': 17797488,\n",
       " 'really': 13342352,\n",
       " 'want': 17593235,\n",
       " 'work': 18281895,\n",
       " 'like': 9378506,\n",
       " 'http': 7616194,\n",
       " 'pinurl': 12539720,\n",
       " 'com': 2923079,\n",
       " 'ukw': 17106883,\n",
       " 'tweetlater pro': 16920587,\n",
       " 'pro way': 12873211,\n",
       " 'way really': 17821171,\n",
       " 'really want': 13404777,\n",
       " 'want work': 17646553,\n",
       " 'work like': 18315164,\n",
       " 'like http': 9426692,\n",
       " 'http pinurl': 7643658,\n",
       " 'pinurl com': 12539721,\n",
       " 'com ukw': 2997748,\n",
       " 'tweetlater pro way': 16920588,\n",
       " 'pro way really': 12873212,\n",
       " 'way really want': 17821254,\n",
       " 'really want work': 13407665,\n",
       " 'want work like': 17646858,\n",
       " 'work like http': 18315321,\n",
       " 'like http pinurl': 9426849,\n",
       " 'http pinurl com': 7643659,\n",
       " 'pinurl com ukw': 12539725,\n",
       " 'tweetlater pro way really': 16920589,\n",
       " 'pro way really want': 12873213,\n",
       " 'way really want work': 17821259,\n",
       " 'really want work like': 13407677,\n",
       " 'want work like http': 17646859,\n",
       " 'work like http pinurl': 18315322,\n",
       " 'like http pinurl com': 9426850,\n",
       " 'http pinurl com ukw': 7643662,\n",
       " 'tweetlater pro way really want': 16920590,\n",
       " 'pro way really want work': 12873214,\n",
       " 'way really want work like': 17821260,\n",
       " 'really want work like http': 13407678,\n",
       " 'want work like http pinurl': 17646860,\n",
       " 'work like http pinurl com': 18315323,\n",
       " 'like http pinurl com ukw': 9426851,\n",
       " 'dudendaeaseonup': 4487506,\n",
       " 'loved': 10062844,\n",
       " 'dudendaeaseonup loved': 4487511,\n",
       " 'amp': 744082,\n",
       " 'repeat': 13539236,\n",
       " 'ipod': 8093699,\n",
       " 'needs': 11388533,\n",
       " 'make': 10274715,\n",
       " 'comeback': 3036096,\n",
       " 'bit': 1750237,\n",
       " 'ly': 10178366,\n",
       " 'stgtu': 15216264,\n",
       " 'amp repeat': 806754,\n",
       " 'repeat ipod': 13539616,\n",
       " 'ipod amp': 8093834,\n",
       " 'amp needs': 797493,\n",
       " 'needs make': 11392802,\n",
       " 'make comeback': 10279777,\n",
       " 'comeback http': 3036139,\n",
       " 'http bit': 7617182,\n",
       " 'bit ly': 1759363,\n",
       " 'ly stgtu': 10194831,\n",
       " 'amp repeat ipod': 806758,\n",
       " 'repeat ipod amp': 13539617,\n",
       " 'ipod amp needs': 8093862,\n",
       " 'amp needs make': 797508,\n",
       " 'needs make comeback': 11392819,\n",
       " 'make comeback http': 10279778,\n",
       " 'comeback http bit': 3036140,\n",
       " 'http bit ly': 7617188,\n",
       " 'bit ly stgtu': 1772915,\n",
       " 'amp repeat ipod amp': 806759,\n",
       " 'repeat ipod amp needs': 13539618,\n",
       " 'ipod amp needs make': 8093863,\n",
       " 'amp needs make comeback': 797509,\n",
       " 'needs make comeback http': 11392820,\n",
       " 'make comeback http bit': 10279779,\n",
       " 'comeback http bit ly': 3036141,\n",
       " 'http bit ly stgtu': 7627838,\n",
       " 'amp repeat ipod amp needs': 806760,\n",
       " 'repeat ipod amp needs make': 13539619,\n",
       " 'ipod amp needs make comeback': 8093864,\n",
       " 'amp needs make comeback http': 797510,\n",
       " 'needs make comeback http bit': 11392821,\n",
       " 'make comeback http bit ly': 10279780,\n",
       " 'comeback http bit ly stgtu': 3036142,\n",
       " 'haha': 6724608,\n",
       " 'miss': 10788871,\n",
       " 'myy': 11244655,\n",
       " 'mattthew': 10487254,\n",
       " 'haha miss': 6746723,\n",
       " 'miss myy': 10813833,\n",
       " 'myy mattthew': 11244826,\n",
       " 'haha miss myy': 6746784,\n",
       " 'miss myy mattthew': 10813837,\n",
       " 'haha miss myy mattthew': 6746785,\n",
       " 'gmorning': 6024355,\n",
       " 'ooh': 12024213,\n",
       " 'giirll': 5939792,\n",
       " 'mondays': 10971940,\n",
       " 'gmorning ooh': 6024445,\n",
       " 'ooh giirll': 12025178,\n",
       " 'giirll mondays': 5939793,\n",
       " 'gmorning ooh giirll': 6024446,\n",
       " 'ooh giirll mondays': 12025179,\n",
       " 'gmorning ooh giirll mondays': 6024447,\n",
       " 'moved': 11097875,\n",
       " '3rd': 201186,\n",
       " 'year': 18667202,\n",
       " 'todayy': 16484307,\n",
       " 'school': 14105740,\n",
       " 'moved 3rd': 11097943,\n",
       " '3rd year': 203292,\n",
       " 'year todayy': 18679626,\n",
       " 'todayy school': 16484521,\n",
       " 'moved 3rd year': 11097947,\n",
       " '3rd year todayy': 203345,\n",
       " 'year todayy school': 18679627,\n",
       " 'moved 3rd year todayy': 11097948,\n",
       " '3rd year todayy school': 203346,\n",
       " 'moved 3rd year todayy school': 11097949,\n",
       " 'home': 7362129,\n",
       " 'summer': 15422133,\n",
       " 'going': 6062854,\n",
       " 'prettyy': 12848842,\n",
       " 'home summer': 7403702,\n",
       " 'summer going': 15426645,\n",
       " 'going prettyy': 6125638,\n",
       " 'home summer going': 7403715,\n",
       " 'summer going prettyy': 15426740,\n",
       " 'home summer going prettyy': 7403718,\n",
       " 'clingermangw': 2823024,\n",
       " 'thanks': 15885249,\n",
       " 'lot': 9948183,\n",
       " 'hell': 7148032,\n",
       " 'compiling': 3084568,\n",
       " 'uploading': 17193439,\n",
       " 'crap': 3313236,\n",
       " 'wait': 17487682,\n",
       " 'till': 16238612,\n",
       " 'alpha': 685895,\n",
       " 'clingermangw thanks': 2823025,\n",
       " 'thanks lot': 15913065,\n",
       " 'lot like': 9954414,\n",
       " 'like hell': 9423698,\n",
       " 'hell compiling': 7149020,\n",
       " 'compiling uploading': 3084637,\n",
       " 'uploading crap': 17193572,\n",
       " 'crap wait': 3319291,\n",
       " 'wait till': 17509475,\n",
       " 'till alpha': 16240332,\n",
       " 'clingermangw thanks lot': 2823026,\n",
       " 'thanks lot like': 15913255,\n",
       " 'lot like hell': 9954472,\n",
       " 'like hell compiling': 9423723,\n",
       " 'hell compiling uploading': 7149021,\n",
       " 'compiling uploading crap': 3084638,\n",
       " 'uploading crap wait': 17193573,\n",
       " 'crap wait till': 3319315,\n",
       " 'wait till alpha': 17509626,\n",
       " 'clingermangw thanks lot like': 2823027,\n",
       " 'thanks lot like hell': 15913256,\n",
       " 'lot like hell compiling': 9954473,\n",
       " 'like hell compiling uploading': 9423724,\n",
       " 'hell compiling uploading crap': 7149022,\n",
       " 'compiling uploading crap wait': 3084639,\n",
       " 'uploading crap wait till': 17193574,\n",
       " 'crap wait till alpha': 3319316,\n",
       " 'clingermangw thanks lot like hell': 2823028,\n",
       " 'thanks lot like hell compiling': 15913257,\n",
       " 'lot like hell compiling uploading': 9954474,\n",
       " 'like hell compiling uploading crap': 9423725,\n",
       " 'hell compiling uploading crap wait': 7149023,\n",
       " 'compiling uploading crap wait till': 3084640,\n",
       " 'uploading crap wait till alpha': 17193575,\n",
       " 'hey': 7205266,\n",
       " 'inforks': 7998037,\n",
       " 'fam': 5026259,\n",
       " 'send': 14247279,\n",
       " 'best': 1602168,\n",
       " 'wishes': 18153850,\n",
       " 'vlad_of_romania': 17444873,\n",
       " 'stomach': 15233467,\n",
       " 'flu': 5404985,\n",
       " 'bad': 1316031,\n",
       " 'hey inforks': 7221197,\n",
       " 'inforks fam': 7998038,\n",
       " 'fam send': 5028259,\n",
       " 'send best': 14247763,\n",
       " 'best wishes': 1624048,\n",
       " 'wishes vlad_of_romania': 18157296,\n",
       " 'vlad_of_romania stomach': 17444874,\n",
       " 'stomach flu': 15234523,\n",
       " 'flu really': 5407605,\n",
       " 'really bad': 13344738,\n",
       " 'hey inforks fam': 7221198,\n",
       " 'inforks fam send': 7998039,\n",
       " 'fam send best': 5028260,\n",
       " 'send best wishes': 14247780,\n",
       " 'best wishes vlad_of_romania': 1624208,\n",
       " 'wishes vlad_of_romania stomach': 18157297,\n",
       " 'vlad_of_romania stomach flu': 17444875,\n",
       " 'stomach flu really': 15234580,\n",
       " 'flu really bad': 5407606,\n",
       " 'hey inforks fam send': 7221199,\n",
       " 'inforks fam send best': 7998040,\n",
       " 'fam send best wishes': 5028261,\n",
       " 'send best wishes vlad_of_romania': 14247783,\n",
       " 'best wishes vlad_of_romania stomach': 1624209,\n",
       " 'wishes vlad_of_romania stomach flu': 18157298,\n",
       " 'vlad_of_romania stomach flu really': 17444876,\n",
       " 'stomach flu really bad': 15234581,\n",
       " 'hey inforks fam send best': 7221200,\n",
       " 'inforks fam send best wishes': 7998041,\n",
       " 'fam send best wishes vlad_of_romania': 5028262,\n",
       " 'send best wishes vlad_of_romania stomach': 14247784,\n",
       " 'best wishes vlad_of_romania stomach flu': 1624210,\n",
       " 'wishes vlad_of_romania stomach flu really': 18157299,\n",
       " 'vlad_of_romania stomach flu really bad': 17444877,\n",
       " 'figpybfo': 5248473,\n",
       " 'ahah': 560238,\n",
       " 'mum': 11184661,\n",
       " 'isn': 8120903,\n",
       " 'figpybfo ahah': 5248474,\n",
       " 'ahah mum': 560601,\n",
       " 'mum isn': 11187091,\n",
       " 'figpybfo ahah mum': 5248475,\n",
       " 'ahah mum isn': 560602,\n",
       " 'figpybfo ahah mum isn': 5248476,\n",
       " 'techno': 15767800,\n",
       " 'party': 12239668,\n",
       " 'took': 16600360,\n",
       " 'place': 12561754,\n",
       " 'room': 13759557,\n",
       " 'night': 11573827,\n",
       " 'completely': 3089830,\n",
       " 'destroyed': 3906661,\n",
       " 'sleep': 14664416,\n",
       " 'techno party': 15767926,\n",
       " 'party took': 12251946,\n",
       " 'took place': 16608354,\n",
       " 'place room': 12568459,\n",
       " 'room night': 13765319,\n",
       " 'night completely': 11581483,\n",
       " 'completely destroyed': 3090565,\n",
       " 'destroyed sleep': 3907017,\n",
       " 'techno party took': 15767930,\n",
       " 'party took place': 12251950,\n",
       " 'took place room': 16608372,\n",
       " 'place room night': 12568464,\n",
       " 'room night completely': 13765326,\n",
       " 'night completely destroyed': 11581484,\n",
       " 'completely destroyed sleep': 3090584,\n",
       " 'techno party took place': 15767931,\n",
       " 'party took place room': 12251951,\n",
       " 'took place room night': 16608373,\n",
       " 'place room night completely': 12568465,\n",
       " 'room night completely destroyed': 13765327,\n",
       " 'night completely destroyed sleep': 11581485,\n",
       " 'techno party took place room': 15767932,\n",
       " 'party took place room night': 12251952,\n",
       " 'took place room night completely': 16608374,\n",
       " 'place room night completely destroyed': 12568466,\n",
       " 'room night completely destroyed sleep': 13765328,\n",
       " 'wrong': 18482351,\n",
       " 'things': 16033892,\n",
       " 'doing': 4199159,\n",
       " 'stuffs': 15350832,\n",
       " 'lame': 9097438,\n",
       " 'write': 18466190,\n",
       " 'affiliations': 506777,\n",
       " 'god': 6030729,\n",
       " 'help': 7172162,\n",
       " 'wrong things': 18489083,\n",
       " 'things doing': 16036943,\n",
       " 'doing stuffs': 4218469,\n",
       " 'stuffs lame': 15350999,\n",
       " 'lame write': 9100132,\n",
       " 'write affiliations': 18466349,\n",
       " 'affiliations god': 506778,\n",
       " 'god help': 6037090,\n",
       " 'wrong things doing': 18489087,\n",
       " 'things doing stuffs': 16036986,\n",
       " 'doing stuffs lame': 4218470,\n",
       " 'stuffs lame write': 15351000,\n",
       " 'lame write affiliations': 9100133,\n",
       " 'write affiliations god': 18466350,\n",
       " 'affiliations god help': 506779,\n",
       " 'wrong things doing stuffs': 18489088,\n",
       " 'things doing stuffs lame': 16036987,\n",
       " 'doing stuffs lame write': 4218471,\n",
       " 'stuffs lame write affiliations': 15351001,\n",
       " 'lame write affiliations god': 9100134,\n",
       " 'write affiliations god help': 18466351,\n",
       " 'wrong things doing stuffs lame': 18489089,\n",
       " 'things doing stuffs lame write': 16036988,\n",
       " 'doing stuffs lame write affiliations': 4218472,\n",
       " 'stuffs lame write affiliations god': 15351002,\n",
       " 'lame write affiliations god help': 9100135,\n",
       " 'radiocolin': 13188650,\n",
       " 'laugh': 9159406,\n",
       " 'flying': 5415497,\n",
       " 'rochester': 13725202,\n",
       " 'ny': 11753100,\n",
       " 'luggage': 10147929,\n",
       " 'went': 17971661,\n",
       " 'manchester': 10383007,\n",
       " 'england': 4730178,\n",
       " 'radiocolin laugh': 13188655,\n",
       " 'laugh flying': 9160046,\n",
       " 'flying rochester': 5416768,\n",
       " 'rochester ny': 13725280,\n",
       " 'ny luggage': 11754458,\n",
       " 'luggage went': 10148245,\n",
       " 'went manchester': 17985219,\n",
       " 'manchester england': 10383259,\n",
       " 'radiocolin laugh flying': 13188656,\n",
       " 'laugh flying rochester': 9160047,\n",
       " 'flying rochester ny': 5416769,\n",
       " 'rochester ny luggage': 13725282,\n",
       " 'ny luggage went': 11754460,\n",
       " 'luggage went manchester': 10148247,\n",
       " 'went manchester england': 17985220,\n",
       " 'radiocolin laugh flying rochester': 13188657,\n",
       " 'laugh flying rochester ny': 9160048,\n",
       " 'flying rochester ny luggage': 5416770,\n",
       " 'rochester ny luggage went': 13725283,\n",
       " 'ny luggage went manchester': 11754461,\n",
       " 'luggage went manchester england': 10148248,\n",
       " 'radiocolin laugh flying rochester ny': 13188658,\n",
       " 'laugh flying rochester ny luggage': 9160049,\n",
       " 'flying rochester ny luggage went': 5416771,\n",
       " 'rochester ny luggage went manchester': 13725284,\n",
       " 'ny luggage went manchester england': 11754462,\n",
       " 'jotac': 8432128,\n",
       " 'thank': 15860757,\n",
       " 'thought': 16155477,\n",
       " 'pic': 12474641,\n",
       " 'looks': 9893524,\n",
       " 'little': 9585837,\n",
       " 'dramatic': 4386342,\n",
       " 'think': 16050495,\n",
       " 'look': 9846950,\n",
       " 'sad': 13864412,\n",
       " 'thinking': 16117956,\n",
       " 'changing': 2573633,\n",
       " 'jotac thank': 8432129,\n",
       " 'thank thought': 15880397,\n",
       " 'thought pic': 16167806,\n",
       " 'pic looks': 12478857,\n",
       " 'looks little': 9905505,\n",
       " 'little dramatic': 9594107,\n",
       " 'dramatic think': 4386522,\n",
       " 'think look': 16087203,\n",
       " 'look little': 9859118,\n",
       " 'little sad': 9605905,\n",
       " 'sad thinking': 13892119,\n",
       " 'thinking changing': 16119657,\n",
       " 'jotac thank thought': 8432130,\n",
       " 'thank thought pic': 15880431,\n",
       " 'thought pic looks': 16167813,\n",
       " 'pic looks little': 12478900,\n",
       " 'looks little dramatic': 9905524,\n",
       " 'little dramatic think': 9594109,\n",
       " 'dramatic think look': 4386523,\n",
       " 'think look little': 16087300,\n",
       " 'look little sad': 9859152,\n",
       " 'little sad thinking': 9606121,\n",
       " 'sad thinking changing': 13892123,\n",
       " 'jotac thank thought pic': 8432131,\n",
       " 'thank thought pic looks': 15880432,\n",
       " 'thought pic looks little': 16167814,\n",
       " 'pic looks little dramatic': 12478901,\n",
       " 'looks little dramatic think': 9905525,\n",
       " 'little dramatic think look': 9594110,\n",
       " 'dramatic think look little': 4386524,\n",
       " 'think look little sad': 16087301,\n",
       " 'look little sad thinking': 9859155,\n",
       " 'little sad thinking changing': 9606122,\n",
       " 'jotac thank thought pic looks': 8432132,\n",
       " 'thank thought pic looks little': 15880433,\n",
       " 'thought pic looks little dramatic': 16167815,\n",
       " 'pic looks little dramatic think': 12478902,\n",
       " 'looks little dramatic think look': 9905526,\n",
       " 'little dramatic think look little': 9594111,\n",
       " 'dramatic think look little sad': 4386525,\n",
       " 'think look little sad thinking': 16087302,\n",
       " 'look little sad thinking changing': 9859156,\n",
       " 'sepinwall': 14272109,\n",
       " 'used': 17250066,\n",
       " 'love': 9981515,\n",
       " 'plcl': 12637382,\n",
       " 'kubiac': 9042058,\n",
       " 'man': 10354121,\n",
       " 'cooler': 3208050,\n",
       " 'grew': 6576489,\n",
       " 'changed': 2569067,\n",
       " 'jerry': 8294836,\n",
       " 'er': 4786531,\n",
       " 'sepinwall used': 14272114,\n",
       " 'used love': 17254652,\n",
       " 'love plcl': 10033869,\n",
       " 'plcl kubiac': 12637383,\n",
       " 'kubiac man': 9042059,\n",
       " 'man cooler': 10357017,\n",
       " 'cooler grew': 3208257,\n",
       " 'grew changed': 6576601,\n",
       " 'changed jerry': 2570195,\n",
       " 'jerry er': 8294944,\n",
       " 'sepinwall used love': 14272115,\n",
       " 'used love plcl': 17254805,\n",
       " 'love plcl kubiac': 10033870,\n",
       " 'plcl kubiac man': 12637384,\n",
       " 'kubiac man cooler': 9042060,\n",
       " 'man cooler grew': 10357018,\n",
       " 'cooler grew changed': 3208258,\n",
       " 'grew changed jerry': 6576602,\n",
       " 'changed jerry er': 2570196,\n",
       " 'sepinwall used love plcl': 14272116,\n",
       " 'used love plcl kubiac': 17254806,\n",
       " 'love plcl kubiac man': 10033871,\n",
       " 'plcl kubiac man cooler': 12637385,\n",
       " 'kubiac man cooler grew': 9042061,\n",
       " 'man cooler grew changed': 10357019,\n",
       " 'cooler grew changed jerry': 3208259,\n",
       " 'grew changed jerry er': 6576603,\n",
       " 'sepinwall used love plcl kubiac': 14272117,\n",
       " 'used love plcl kubiac man': 17254807,\n",
       " 'love plcl kubiac man cooler': 10033872,\n",
       " 'plcl kubiac man cooler grew': 12637386,\n",
       " 'kubiac man cooler grew changed': 9042062,\n",
       " 'man cooler grew changed jerry': 10357020,\n",
       " 'cooler grew changed jerry er': 3208260,\n",
       " 'twitpic': 16950324,\n",
       " '7g5jh': 302500,\n",
       " 'lotion': 9961931,\n",
       " 'does': 4147764,\n",
       " 'wonders': 18244173,\n",
       " 'http twitpic': 7659567,\n",
       " 'twitpic com': 16950505,\n",
       " 'com 7g5jh': 2963091,\n",
       " '7g5jh lotion': 302501,\n",
       " 'lotion does': 9961976,\n",
       " 'does wonders': 4163967,\n",
       " 'http twitpic com': 7659568,\n",
       " 'twitpic com 7g5jh': 16979099,\n",
       " 'com 7g5jh lotion': 2963092,\n",
       " '7g5jh lotion does': 302502,\n",
       " 'lotion does wonders': 9961977,\n",
       " 'http twitpic com 7g5jh': 7679291,\n",
       " 'twitpic com 7g5jh lotion': 16979100,\n",
       " 'com 7g5jh lotion does': 2963093,\n",
       " '7g5jh lotion does wonders': 302503,\n",
       " 'http twitpic com 7g5jh lotion': 7679292,\n",
       " 'twitpic com 7g5jh lotion does': 16979101,\n",
       " 'com 7g5jh lotion does wonders': 2963094,\n",
       " 'dopegirlfresh': 4343143,\n",
       " 'titty': 16386111,\n",
       " 'grab': 6480392,\n",
       " 'feel': 5133995,\n",
       " 'better': 1638894,\n",
       " 'dopegirlfresh let': 4343159,\n",
       " 'let titty': 9312917,\n",
       " 'titty grab': 16386133,\n",
       " 'grab feel': 6480794,\n",
       " 'feel better': 5137280,\n",
       " 'dopegirlfresh let titty': 4343160,\n",
       " 'let titty grab': 9312918,\n",
       " 'titty grab feel': 16386134,\n",
       " 'grab feel better': 6480795,\n",
       " 'dopegirlfresh let titty grab': 4343161,\n",
       " 'let titty grab feel': 9312919,\n",
       " 'titty grab feel better': 16386135,\n",
       " 'dopegirlfresh let titty grab feel': 4343162,\n",
       " 'let titty grab feel better': 9312920,\n",
       " 'mistawilly': 10877979,\n",
       " 'happy': 6871897,\n",
       " 'bday': 1459974,\n",
       " 'hope': 7445634,\n",
       " 'good': 6214254,\n",
       " 'mistawilly happy': 10877982,\n",
       " 'happy bday': 6873976,\n",
       " 'bday hope': 1461559,\n",
       " 'hope good': 7461564,\n",
       " 'mistawilly happy bday': 10877983,\n",
       " 'happy bday hope': 6874194,\n",
       " 'bday hope good': 1461567,\n",
       " 'mistawilly happy bday hope': 10877984,\n",
       " 'happy bday hope good': 6874199,\n",
       " 'mistawilly happy bday hope good': 10877985,\n",
       " 'cops': 3214821,\n",
       " 'looking': 9873092,\n",
       " 'mr': 11133653,\n",
       " 'pickering': 12490868,\n",
       " 'cops looking': 3215057,\n",
       " 'looking mr': 9887632,\n",
       " 'mr pickering': 11136947,\n",
       " 'cops looking mr': 3215058,\n",
       " 'looking mr pickering': 9887639,\n",
       " 'cops looking mr pickering': 3215059,\n",
       " 'prob': 12873538,\n",
       " 'nemosocean': 11408498,\n",
       " 'ur': 17204329,\n",
       " 'awesome': 1198090,\n",
       " 'talentedgeniu5': 15665630,\n",
       " 'hokie': 7338691,\n",
       " 'pokie': 12684945,\n",
       " 'just': 8488147,\n",
       " 'dance': 3540171,\n",
       " 'thats': 15940758,\n",
       " 'prob nemosocean': 12875079,\n",
       " 'nemosocean ur': 11408503,\n",
       " 'ur awesome': 17205435,\n",
       " 'awesome talentedgeniu5': 1217173,\n",
       " 'talentedgeniu5 hokie': 15665635,\n",
       " 'hokie pokie': 7338694,\n",
       " 'pokie just': 12684946,\n",
       " 'just dance': 8518783,\n",
       " 'dance thats': 3544672,\n",
       " 'prob nemosocean ur': 12875080,\n",
       " 'nemosocean ur awesome': 11408504,\n",
       " 'ur awesome talentedgeniu5': 17205475,\n",
       " 'awesome talentedgeniu5 hokie': 1217174,\n",
       " 'talentedgeniu5 hokie pokie': 15665636,\n",
       " 'hokie pokie just': 7338695,\n",
       " 'pokie just dance': 12684947,\n",
       " 'just dance thats': 8518862,\n",
       " 'prob nemosocean ur awesome': 12875081,\n",
       " 'nemosocean ur awesome talentedgeniu5': 11408505,\n",
       " 'ur awesome talentedgeniu5 hokie': 17205476,\n",
       " 'awesome talentedgeniu5 hokie pokie': 1217175,\n",
       " 'talentedgeniu5 hokie pokie just': 15665637,\n",
       " 'hokie pokie just dance': 7338696,\n",
       " 'pokie just dance thats': 12684948,\n",
       " 'prob nemosocean ur awesome talentedgeniu5': 12875082,\n",
       " 'nemosocean ur awesome talentedgeniu5 hokie': 11408506,\n",
       " 'ur awesome talentedgeniu5 hokie pokie': 17205477,\n",
       " 'awesome talentedgeniu5 hokie pokie just': 1217176,\n",
       " 'talentedgeniu5 hokie pokie just dance': 15665638,\n",
       " 'hokie pokie just dance thats': 7338697,\n",
       " 'lemongeneration': 9281401,\n",
       " 'hahahaha': 6780540,\n",
       " 'meal': 10538759,\n",
       " 'veggie': 17362849,\n",
       " 'edition': 4613352,\n",
       " 'lemongeneration hahahaha': 9281452,\n",
       " 'hahahaha does': 6780868,\n",
       " 'does happy': 4152800,\n",
       " 'happy meal': 6890723,\n",
       " 'meal veggie': 10539913,\n",
       " 'veggie edition': 17362989,\n",
       " 'lemongeneration hahahaha does': 9281453,\n",
       " 'hahahaha does happy': 6780869,\n",
       " 'does happy meal': 4152812,\n",
       " 'happy meal veggie': 6890734,\n",
       " 'meal veggie edition': 10539914,\n",
       " 'lemongeneration hahahaha does happy': 9281454,\n",
       " 'hahahaha does happy meal': 6780870,\n",
       " 'does happy meal veggie': 4152813,\n",
       " 'happy meal veggie edition': 6890735,\n",
       " 'lemongeneration hahahaha does happy meal': 9281455,\n",
       " 'hahahaha does happy meal veggie': 6780871,\n",
       " 'does happy meal veggie edition': 4152814,\n",
       " 'ngebay': 11514979,\n",
       " 'yeahh': 18664777,\n",
       " 'cuz': 3452767,\n",
       " 'workin': 18357731,\n",
       " 'late': 9129161,\n",
       " 'nite': 11648405,\n",
       " 'ngebay yeahh': 11514980,\n",
       " 'yeahh cuz': 18664917,\n",
       " 'cuz workin': 3462261,\n",
       " 'workin till': 18359090,\n",
       " 'till late': 16244286,\n",
       " 'late nite': 9137164,\n",
       " 'ngebay yeahh cuz': 11514981,\n",
       " 'yeahh cuz workin': 18664918,\n",
       " 'cuz workin till': 3462262,\n",
       " 'workin till late': 18359109,\n",
       " 'till late nite': 16244335,\n",
       " 'ngebay yeahh cuz workin': 11514982,\n",
       " 'yeahh cuz workin till': 18664919,\n",
       " 'cuz workin till late': 3462263,\n",
       " 'workin till late nite': 18359110,\n",
       " 'ngebay yeahh cuz workin till': 11514983,\n",
       " 'yeahh cuz workin till late': 18664920,\n",
       " 'cuz workin till late nite': 3462264,\n",
       " 'mamajoan': 10352001,\n",
       " 'suck': 15381647,\n",
       " 'right': 13645455,\n",
       " 'mamajoan suck': 10352002,\n",
       " 'suck right': 15384778,\n",
       " 'mamajoan suck right': 10352003,\n",
       " 'base': 1416129,\n",
       " 'training': 16696793,\n",
       " 'base training': 1416595,\n",
       " 'deancoulson': 3781358,\n",
       " 'pain': 12169277,\n",
       " 'ah': 548309,\n",
       " 'trick': 16734051,\n",
       " 'deancoulson know': 3781375,\n",
       " 'know pain': 8973379,\n",
       " 'pain ah': 12169409,\n",
       " 'ah trick': 556991,\n",
       " 'trick training': 16734543,\n",
       " 'training home': 16697824,\n",
       " 'deancoulson know pain': 3781376,\n",
       " 'know pain ah': 8973383,\n",
       " 'pain ah trick': 12169413,\n",
       " 'ah trick training': 556992,\n",
       " 'trick training home': 16734544,\n",
       " 'deancoulson know pain ah': 3781377,\n",
       " 'know pain ah trick': 8973384,\n",
       " 'pain ah trick training': 12169414,\n",
       " 'ah trick training home': 556993,\n",
       " 'deancoulson know pain ah trick': 3781378,\n",
       " 'know pain ah trick training': 8973385,\n",
       " 'pain ah trick training home': 12169415,\n",
       " 'fun': 5700819,\n",
       " 'dry': 4467377,\n",
       " 'socket': 14800036,\n",
       " 'having': 6998620,\n",
       " 'wisdom': 18110997,\n",
       " 'pulled': 12968780,\n",
       " 'guess': 6623145,\n",
       " 'means': 10554005,\n",
       " 'meds': 10574608,\n",
       " 'fun fun': 5709634,\n",
       " 'fun dry': 5707421,\n",
       " 'dry socket': 4468683,\n",
       " 'socket having': 14800057,\n",
       " 'having wisdom': 7023487,\n",
       " 'wisdom pulled': 18111238,\n",
       " 'pulled guess': 12969311,\n",
       " 'guess means': 6633504,\n",
       " 'means pain': 10558011,\n",
       " 'pain meds': 12172623,\n",
       " 'fun fun dry': 5709692,\n",
       " 'fun dry socket': 5707422,\n",
       " 'dry socket having': 4468685,\n",
       " 'socket having wisdom': 14800058,\n",
       " 'having wisdom pulled': 7023488,\n",
       " 'wisdom pulled guess': 18111239,\n",
       " 'pulled guess means': 12969314,\n",
       " 'guess means pain': 6633662,\n",
       " 'means pain meds': 10558012,\n",
       " 'fun fun dry socket': 5709693,\n",
       " 'fun dry socket having': 5707423,\n",
       " 'dry socket having wisdom': 4468686,\n",
       " 'socket having wisdom pulled': 14800059,\n",
       " 'having wisdom pulled guess': 7023489,\n",
       " 'wisdom pulled guess means': 18111240,\n",
       " 'pulled guess means pain': 12969315,\n",
       " 'guess means pain meds': 6633663,\n",
       " 'fun fun dry socket having': 5709694,\n",
       " 'fun dry socket having wisdom': 5707424,\n",
       " 'dry socket having wisdom pulled': 4468687,\n",
       " 'socket having wisdom pulled guess': 14800060,\n",
       " 'having wisdom pulled guess means': 7023490,\n",
       " 'wisdom pulled guess means pain': 18111241,\n",
       " 'pulled guess means pain meds': 12969316,\n",
       " 'anitavlachos': 879132,\n",
       " 'mi': 10672904,\n",
       " 'wow': 18437785,\n",
       " 'great': 6515526,\n",
       " 'hear': 7077771,\n",
       " 'chance': 2556525,\n",
       " 'outside': 12105879,\n",
       " 'savor': 14015424,\n",
       " 'anitavlachos mi': 879137,\n",
       " 'mi wow': 10674728,\n",
       " 'wow great': 18444670,\n",
       " 'great hear': 6531339,\n",
       " 'hear hope': 7083128,\n",
       " 'hope chance': 7449710,\n",
       " 'chance outside': 2558992,\n",
       " 'outside amp': 12106188,\n",
       " 'amp savor': 809408,\n",
       " 'anitavlachos mi wow': 879138,\n",
       " 'mi wow great': 10674729,\n",
       " 'wow great hear': 18444746,\n",
       " 'great hear hope': 6531426,\n",
       " 'hear hope chance': 7083146,\n",
       " 'hope chance outside': 7449732,\n",
       " 'chance outside amp': 2558993,\n",
       " 'outside amp savor': 12106239,\n",
       " 'anitavlachos mi wow great': 879139,\n",
       " 'mi wow great hear': 10674730,\n",
       " 'wow great hear hope': 18444747,\n",
       " 'great hear hope chance': 6531427,\n",
       " 'hear hope chance outside': 7083147,\n",
       " 'hope chance outside amp': 7449733,\n",
       " 'chance outside amp savor': 2558994,\n",
       " 'anitavlachos mi wow great hear': 879140,\n",
       " 'mi wow great hear hope': 10674731,\n",
       " 'wow great hear hope chance': 18444748,\n",
       " 'great hear hope chance outside': 6531428,\n",
       " 'hear hope chance outside amp': 7083148,\n",
       " 'hope chance outside amp savor': 7449734,\n",
       " '7gezc': 303059,\n",
       " 'maritzamendoza': 10420596,\n",
       " 'order': 12067522,\n",
       " 'needed': 11383971,\n",
       " 'rainboots': 13209112,\n",
       " 'online': 12012286,\n",
       " 'lack': 9067453,\n",
       " 'store': 15258367,\n",
       " 'options': 12061980,\n",
       " 'com 7gezc': 2963546,\n",
       " '7gezc maritzamendoza': 303060,\n",
       " 'maritzamendoza ended': 10420597,\n",
       " 'ended having': 4718353,\n",
       " 'having order': 7015688,\n",
       " 'order needed': 12069435,\n",
       " 'needed rainboots': 11386137,\n",
       " 'rainboots online': 13209125,\n",
       " 'online lack': 12015252,\n",
       " 'lack store': 9068799,\n",
       " 'store options': 15261769,\n",
       " 'twitpic com 7gezc': 16979452,\n",
       " 'com 7gezc maritzamendoza': 2963547,\n",
       " '7gezc maritzamendoza ended': 303061,\n",
       " 'maritzamendoza ended having': 10420598,\n",
       " 'ended having order': 4718416,\n",
       " 'having order needed': 7015689,\n",
       " 'order needed rainboots': 12069439,\n",
       " 'needed rainboots online': 11386138,\n",
       " 'rainboots online lack': 13209126,\n",
       " 'online lack store': 12015253,\n",
       " 'lack store options': 9068800,\n",
       " 'http twitpic com 7gezc': 7679537,\n",
       " 'twitpic com 7gezc maritzamendoza': 16979453,\n",
       " 'com 7gezc maritzamendoza ended': 2963548,\n",
       " '7gezc maritzamendoza ended having': 303062,\n",
       " 'maritzamendoza ended having order': 10420599,\n",
       " 'ended having order needed': 4718417,\n",
       " 'having order needed rainboots': 7015690,\n",
       " 'order needed rainboots online': 12069440,\n",
       " 'needed rainboots online lack': 11386139,\n",
       " 'rainboots online lack store': 13209127,\n",
       " 'online lack store options': 12015254,\n",
       " 'http twitpic com 7gezc maritzamendoza': 7679538,\n",
       " 'twitpic com 7gezc maritzamendoza ended': 16979454,\n",
       " 'com 7gezc maritzamendoza ended having': 2963549,\n",
       " '7gezc maritzamendoza ended having order': 303063,\n",
       " 'maritzamendoza ended having order needed': 10420600,\n",
       " 'ended having order needed rainboots': 4718418,\n",
       " 'having order needed rainboots online': 7015691,\n",
       " 'order needed rainboots online lack': 12069441,\n",
       " 'needed rainboots online lack store': 11386140,\n",
       " 'rainboots online lack store options': 13209128,\n",
       " 'thisstarchild': 16140723,\n",
       " 'did': 3936347,\n",
       " 'thisstarchild did': 16140740,\n",
       " 'did miss': 3959201,\n",
       " 'miss fun': 10801463,\n",
       " 'thisstarchild did miss': 16140741,\n",
       " 'did miss fun': 3959352,\n",
       " 'thisstarchild did miss fun': 16140742,\n",
       " 'lots': 9962423,\n",
       " 'rides': 13640382,\n",
       " 'today': 16402653,\n",
       " 'sick': 14505651,\n",
       " 'lots fun': 9964794,\n",
       " 'fun rides': 5722127,\n",
       " 'rides today': 13640794,\n",
       " 'today feel': 16423438,\n",
       " 'feel sick': 5163523,\n",
       " 'lots fun rides': 9965061,\n",
       " 'fun rides today': 5722128,\n",
       " 'rides today feel': 13640795,\n",
       " 'today feel sick': 16423701,\n",
       " 'lots fun rides today': 9965062,\n",
       " 'fun rides today feel': 5722129,\n",
       " 'rides today feel sick': 13640796,\n",
       " 'lots fun rides today feel': 9965063,\n",
       " 'fun rides today feel sick': 5722130,\n",
       " 'danjite': 3564162,\n",
       " 'welcome': 17956235,\n",
       " 'borrow': 1975216,\n",
       " 'time': 16252617,\n",
       " 'masterton': 10460095,\n",
       " 'danjite welcome': 3564163,\n",
       " 'welcome borrow': 17956899,\n",
       " 'borrow time': 1975734,\n",
       " 'time masterton': 16289390,\n",
       " 'danjite welcome borrow': 3564164,\n",
       " 'welcome borrow time': 17956900,\n",
       " 'borrow time masterton': 1975735,\n",
       " 'danjite welcome borrow time': 3564165,\n",
       " 'welcome borrow time masterton': 17956901,\n",
       " 'danjite welcome borrow time masterton': 3564166,\n",
       " 'headache': 7050156,\n",
       " 'oc': 11776608,\n",
       " 'tomorrow': 16522296,\n",
       " 'headache night': 7053651,\n",
       " 'night sleep': 11612933,\n",
       " 'sleep oc': 14684724,\n",
       " 'oc tomorrow': 11776962,\n",
       " 'headache night sleep': 7053677,\n",
       " 'night sleep oc': 11613281,\n",
       " 'sleep oc tomorrow': 14684725,\n",
       " 'headache night sleep oc': 7053678,\n",
       " 'night sleep oc tomorrow': 11613282,\n",
       " 'headache night sleep oc tomorrow': 7053679,\n",
       " 'bb': 1442639,\n",
       " 'wrong bb': 18482836,\n",
       " 'tommcfly': 16510427,\n",
       " 'amazing': 708279,\n",
       " 'concert': 3110372,\n",
       " 'chile': 2678027,\n",
       " 'tommcfly awesome': 16510787,\n",
       " 'awesome amazing': 1198713,\n",
       " 'amazing love': 716343,\n",
       " 'love concert': 9993791,\n",
       " 'concert hope': 3112715,\n",
       " 'hope like': 7468707,\n",
       " 'like chile': 9395492,\n",
       " 'tommcfly awesome amazing': 16510788,\n",
       " 'awesome amazing love': 1198740,\n",
       " 'amazing love concert': 716407,\n",
       " 'love concert hope': 9993800,\n",
       " 'concert hope like': 3112734,\n",
       " 'hope like chile': 7468741,\n",
       " 'tommcfly awesome amazing love': 16510789,\n",
       " 'awesome amazing love concert': 1198741,\n",
       " 'amazing love concert hope': 716410,\n",
       " 'love concert hope like': 9993801,\n",
       " 'concert hope like chile': 3112735,\n",
       " 'tommcfly awesome amazing love concert': 16510790,\n",
       " 'awesome amazing love concert hope': 1198742,\n",
       " 'amazing love concert hope like': 716411,\n",
       " 'love concert hope like chile': 9993802,\n",
       " 'charlielevin': 2592520,\n",
       " 'rooting': 13772309,\n",
       " 'ya': 18560999,\n",
       " 'kick': 8809949,\n",
       " 'ass': 1073387,\n",
       " 'charlielevin rooting': 2592521,\n",
       " 'rooting ya': 13772626,\n",
       " 'ya kick': 18566544,\n",
       " 'kick ass': 8810051,\n",
       " 'charlielevin rooting ya': 2592522,\n",
       " 'rooting ya kick': 13772628,\n",
       " 'ya kick ass': 18566545,\n",
       " 'charlielevin rooting ya kick': 2592523,\n",
       " 'rooting ya kick ass': 13772629,\n",
       " 'charlielevin rooting ya kick ass': 2592524,\n",
       " 'vivalatrace': 17442322,\n",
       " 'oh': 11813875,\n",
       " 'pissed': 12543610,\n",
       " 'vivalatrace oh': 17442330,\n",
       " 'oh know': 11843846,\n",
       " 'know pissed': 8974838,\n",
       " 'vivalatrace oh know': 17442331,\n",
       " 'oh know pissed': 11844178,\n",
       " 'vivalatrace oh know pissed': 17442332,\n",
       " 'says': 14069252,\n",
       " 'morning': 11024856,\n",
       " 'plurk': 12650316,\n",
       " 'yqy4f': 18799124,\n",
       " 'says good': 14072521,\n",
       " 'good morning': 6263205,\n",
       " 'morning http': 11042909,\n",
       " 'http plurk': 7643718,\n",
       " 'plurk com': 12650401,\n",
       " 'com yqy4f': 3002278,\n",
       " 'says good morning': 14072605,\n",
       " 'good morning http': 6267071,\n",
       " 'morning http plurk': 11042989,\n",
       " 'http plurk com': 7643719,\n",
       " 'plurk com yqy4f': 12654352,\n",
       " 'says good morning http': 14072638,\n",
       " 'good morning http plurk': 6267083,\n",
       " 'morning http plurk com': 11042990,\n",
       " 'http plurk com yqy4f': 7647611,\n",
       " 'says good morning http plurk': 14072639,\n",
       " 'good morning http plurk com': 6267084,\n",
       " 'morning http plurk com yqy4f': 11043054,\n",
       " 'new': 11430872,\n",
       " 'drugs': 4459980,\n",
       " 'choci': 2702399,\n",
       " 'gnosischocolate': 6027174,\n",
       " 'squarespace': 15077447,\n",
       " 'new drugs': 11445874,\n",
       " 'drugs choci': 4460068,\n",
       " 'choci http': 2702400,\n",
       " 'http gnosischocolate': 7637447,\n",
       " 'gnosischocolate squarespace': 6027177,\n",
       " 'squarespace com': 15077539,\n",
       " 'new drugs choci': 11445875,\n",
       " 'drugs choci http': 4460069,\n",
       " 'choci http gnosischocolate': 2702401,\n",
       " 'http gnosischocolate squarespace': 7637448,\n",
       " 'gnosischocolate squarespace com': 6027178,\n",
       " 'new drugs choci http': 11445876,\n",
       " 'drugs choci http gnosischocolate': 4460070,\n",
       " 'choci http gnosischocolate squarespace': 2702402,\n",
       " 'http gnosischocolate squarespace com': 7637449,\n",
       " 'new drugs choci http gnosischocolate': 11445877,\n",
       " 'drugs choci http gnosischocolate squarespace': 4460071,\n",
       " 'choci http gnosischocolate squarespace com': 2702403,\n",
       " 'fragilebubble': 5551026,\n",
       " 'thanx': 15936247,\n",
       " 'hun': 7724031,\n",
       " 'yeah': 18624697,\n",
       " 'hearing': 7101788,\n",
       " 'real': 13316326,\n",
       " 'encouraging': 4706336,\n",
       " 'yesterday': 18741640,\n",
       " 'finances': 5301225,\n",
       " 'screwed': 14160561,\n",
       " 'hubby': 7695093,\n",
       " 'wages': 17484095,\n",
       " 'fragilebubble thanx': 5551047,\n",
       " 'thanx hun': 15937087,\n",
       " 'hun yeah': 7726534,\n",
       " 'yeah know': 18642853,\n",
       " 'know hearing': 8953749,\n",
       " 'hearing real': 7103138,\n",
       " 'real encouraging': 13318558,\n",
       " 'encouraging yesterday': 4706507,\n",
       " 'yesterday finances': 18745902,\n",
       " 'finances work': 5301319,\n",
       " 'work screwed': 18330379,\n",
       " 'screwed hubby': 14161070,\n",
       " 'hubby wages': 7698706,\n",
       " 'fragilebubble thanx hun': 5551048,\n",
       " 'thanx hun yeah': 15937099,\n",
       " 'hun yeah know': 7726538,\n",
       " 'yeah know hearing': 18643242,\n",
       " 'know hearing real': 8953759,\n",
       " 'hearing real encouraging': 7103139,\n",
       " 'real encouraging yesterday': 13318559,\n",
       " 'encouraging yesterday finances': 4706508,\n",
       " 'yesterday finances work': 18745903,\n",
       " 'finances work screwed': 5301320,\n",
       " 'work screwed hubby': 18330380,\n",
       " 'screwed hubby wages': 14161071,\n",
       " 'fragilebubble thanx hun yeah': 5551049,\n",
       " 'thanx hun yeah know': 15937100,\n",
       " 'hun yeah know hearing': 7726539,\n",
       " 'yeah know hearing real': 18643243,\n",
       " 'know hearing real encouraging': 8953760,\n",
       " 'hearing real encouraging yesterday': 7103140,\n",
       " 'real encouraging yesterday finances': 13318560,\n",
       " 'encouraging yesterday finances work': 4706509,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trunc = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "x_train_trunc_vectorizer=count_trunc.fit_transform(x_train_trunc)\n",
    "x_test_trunc_vectorizer=count_trunc.transform(x_test_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**XGBoost**](##XGBoost)\n",
    "* [**Keras**](##Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm used for classification and regression tasks. It combines multiple weak models, typically decision trees, to create a stronger and more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the intention to employ a neural network for text classification, an XGBoost model has been trained to evaluate its performance and compare it with the results of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with vectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 791  803]\n",
      " [ 208 1398]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.50      0.61      1594\n",
      "           1       0.64      0.87      0.73      1606\n",
      "\n",
      "    accuracy                           0.68      3200\n",
      "   macro avg       0.71      0.68      0.67      3200\n",
      "weighted avg       0.71      0.68      0.67      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model_vectorizer = xgb_model.fit(x_train_trunc_vectorizer, y_train_trunc)\n",
    "xgb_predictions_vectorizer=xgb_model_vectorizer.predict(x_test_trunc_vectorizer)\n",
    "print(confusion_matrix(y_test_trunc,xgb_predictions_vectorizer))\n",
    "print (classification_report(y_test_trunc, xgb_predictions_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is being processed using the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 300\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Embedding Layer:** This layer converts input words into dense vectors of fixed length. The vocabulary size is set to 50,000 words, and each word is represented as a 100-dimensional vector. The input size of this layer is a maximum length of 300 words.\n",
    "\n",
    "* **Spatial Dropout1D Layer:** This layer applies dropout to prevent overfitting. It randomly drops out entire channels (feature maps) instead of individual neurons. In this case, 20% of the outputs from the Embedding layer are randomly set to 0.\n",
    "\n",
    "* **LSTM Layer:** This layer utilizes LSTM (Long Short-Term Memory) units to model the sequence of words in the text. Each LSTM unit has 100 memory cells and can capture long-term patterns in sequential data. The LSTM layer also applies dropout with a rate of 20% on the recurrent connections to prevent overfitting.\n",
    "\n",
    "* **Dense Layer:** This is the output layer of the model, consisting of a single neuron with a sigmoid activation function. It produces an output between 0 and 1, representing the probability of the text instance belonging to a particular class (e.g., positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ready\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m5,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,080,501</span> (19.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,080,501\u001b[0m (19.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,080,501</span> (19.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,080,501\u001b[0m (19.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Construye el grafo computacional\n",
    "model.build(input_shape=(None, max_len))\n",
    "\n",
    "# Imprime el resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EarlyStopping:**\n",
    "* monitor='val_accuracy': It monitors the validation accuracy during training.\n",
    "* mode='max': It maximizes the monitored metric (validation accuracy in this case).\n",
    "* patience=5: It specifies the number of epochs to wait before stopping the training process if the monitored metric doesn't improve.\n",
    "\n",
    "<br>\n",
    "\n",
    "**ModelCheckpoint:**\n",
    "* filepath='./keras': It specifies the path and filename to save the model weights.\n",
    "* save_weights_only=True: It indicates that only the weights of the best model will be saved, not the entire model.\n",
    "* monitor='val_accuracy': It monitors the validation accuracy during training.\n",
    "* mode='max': It maximizes the monitored metric (validation accuracy in this case).\n",
    "* save_best_only=True: It saves only the weights of the best model based on the monitored metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    mode='max',\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "checkpoint= ModelCheckpoint(\n",
    "    filepath='./keras_weights.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2234s\u001b[0m 2s/step - accuracy: 0.7004 - loss: 0.5596 - val_accuracy: 0.7707 - val_loss: 0.4884\n",
      "Epoch 2/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2076s\u001b[0m 2s/step - accuracy: 0.7750 - loss: 0.4763 - val_accuracy: 0.7742 - val_loss: 0.4753\n",
      "Epoch 3/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2820s\u001b[0m 2s/step - accuracy: 0.7812 - loss: 0.4631 - val_accuracy: 0.7792 - val_loss: 0.4637\n",
      "Epoch 4/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1978s\u001b[0m 2s/step - accuracy: 0.7855 - loss: 0.4551 - val_accuracy: 0.7851 - val_loss: 0.4569\n",
      "Epoch 5/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1993s\u001b[0m 2s/step - accuracy: 0.7886 - loss: 0.4496 - val_accuracy: 0.7867 - val_loss: 0.4591\n",
      "Epoch 6/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1980s\u001b[0m 2s/step - accuracy: 0.7928 - loss: 0.4428 - val_accuracy: 0.7908 - val_loss: 0.4527\n",
      "Epoch 7/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2005s\u001b[0m 2s/step - accuracy: 0.7952 - loss: 0.4388 - val_accuracy: 0.7923 - val_loss: 0.4503\n",
      "Epoch 8/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2030s\u001b[0m 2s/step - accuracy: 0.7970 - loss: 0.4364 - val_accuracy: 0.7916 - val_loss: 0.4471\n",
      "Epoch 9/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2037s\u001b[0m 2s/step - accuracy: 0.7987 - loss: 0.4315 - val_accuracy: 0.7893 - val_loss: 0.4496\n",
      "Epoch 10/10\n",
      "\u001b[1m1161/1161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2041s\u001b[0m 2s/step - accuracy: 0.8014 - loss: 0.4283 - val_accuracy: 0.7896 - val_loss: 0.4483\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights('./keras_weights.weights.h5')\n",
    "\n",
    "history=model.fit(sequences_matrix,y_train,batch_size=1024,epochs=10,\n",
    "            validation_split=0.01,callbacks=[stop,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "load_model=keras.models.load_model(\"./sentiment_model.h5\")\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello boy']\n",
      "[[]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "pred [[0.45266718]]\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "test = 'hello boy'\n",
    "\n",
    "test=[clean(test)]\n",
    "print(test)\n",
    "seq = load_tokenizer.texts_to_sequences([test])\n",
    "padded = sequence.pad_sequences(seq, maxlen=300)\n",
    "print(seq)\n",
    "pred = load_model.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if 0.43 < pred < 0.53:\n",
    "    print(\"neutral\")\n",
    "elif pred > 0.50:\n",
    "    print(\"positive\")\n",
    "else:\n",
    "    print(\"negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

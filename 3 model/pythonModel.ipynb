{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/javier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "stopword=set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,SpatialDropout1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1. [Importing Data](#Data)\n",
    "2. [Data Analysis](../Analysis/train_data_analysis.ipynb)\n",
    "3. [Embeddings and Text Cleaning](#Embeddings-and-Text-Cleaning)\n",
    "4. [Preprocessing](#Preprocessing)\n",
    "5. [Models](#Models)\n",
    "6. [Real Test](#Real-Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_paths = [\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part1',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part2',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part3',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part4',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part5',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part6',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part7',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part8',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part9',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part10',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part11',\n",
    "    '../data/kaggle/sentiment/training.1600000.processed.noemoticon.csv.part12',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(part_path, encoding='utf-8', lineterminator='\\n', on_bad_lines='skip') for part_path in part_paths]\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date     query             user                                               text\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all....\n",
       "...         ...         ...                           ...       ...              ...                                                ...\n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY  AmandaMarie1028  Just woke up. Having no school is the best fee...\n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY           bpbabe  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean(text)` function takes a text as input and performs a series of transformations to clean and normalize the text. Here is a brief explanation of what each step does:\n",
    "\n",
    "* Converts the text to lowercase.\n",
    "* Removes special characters and replaces specific characters with others.\n",
    "* Replaces common contractions with their full forms.\n",
    "* Replaces character entity references such as \">\" and \"<\".\n",
    "* Corrects typos, slang, and informal abbreviations.\n",
    "* Replaces hashtags and social media usernames with their full forms or descriptions.\n",
    "* Cleans and normalizes words related to natural disasters, current events, and other specific words.\n",
    "* Performs additional corrections and normalizations in the text.\n",
    "* Removes URLs starting with \"http://\" or \"https://\" followed by alphanumeric characters.\n",
    "* Replaces each punctuation and special character with a space and the punctuation/special character itself.\n",
    "* Replaces specific acronyms with their expanded forms or related terms.\n",
    "* Removes Tags, Links, and Punctuation.\n",
    "* Does Stopword Removal and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text): \n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Special characters\n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "    text = re.sub(r\"donå«t\", \"do not\", text)   \n",
    "            \n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", text)\n",
    "    text = re.sub(r\"recentlu\", \"recently\", text)\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n",
    "    text = re.sub(r\"amirite\", \"am I right\", text)\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", text)\n",
    "    text = re.sub(r\"8/5/2015\", \"2015-08-05\", text)\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n",
    "    text = re.sub(r\"8/6/2015\", \"2015-08-06\", text)\n",
    "    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n",
    "    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)   \n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n",
    "    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n",
    "    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n",
    "    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n",
    "    text = re.sub(r\"MTVHottest\", \"MTV Hottest\", text)\n",
    "    text = re.sub(r\"TrapMusic\", \"Trap Music\", text)\n",
    "    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n",
    "    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n",
    "    text = re.sub(r\"socialnews\", \"social news\", text)\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"onlinecommunities\", \"online communities\", text)\n",
    "    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n",
    "    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n",
    "    text = re.sub(r\"Meat-Loving\", \"Meat Loving\", text)\n",
    "    text = re.sub(r\"facialabuse\", \"facial abuse\", text)\n",
    "    text = re.sub(r\"LakeCounty\", \"Lake County\", text)\n",
    "    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n",
    "    text = re.sub(r\"withheavenly\", \"with heavenly\", text)\n",
    "    text = re.sub(r\"thankU\", \"thank you\", text)\n",
    "    text = re.sub(r\"iTunesMusic\", \"iTunes Music\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n",
    "    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n",
    "    text = re.sub(r\"animalrescue\", \"animal rescue\", text)\n",
    "    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text)\n",
    "    text = re.sub(r\"aRmageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n",
    "    text = re.sub(r\"GodsLove\", \"God's Love\", text)\n",
    "    text = re.sub(r\"bookboost\", \"book boost\", text)\n",
    "    text = re.sub(r\"ibooklove\", \"I book love\", text)\n",
    "    text = re.sub(r\"NestleIndia\", \"Nestle India\", text)\n",
    "    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\", text)\n",
    "    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text)\n",
    "    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text)\n",
    "    text = re.sub(r\"weathernetwork\", \"weather network\", text)\n",
    "    text = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", text)\n",
    "    text = re.sub(r\"Hostage&2\", \"Hostage & 2\", text)\n",
    "    text = re.sub(r\"GOPDebate\", \"GOP Debate\", text)\n",
    "    text = re.sub(r\"RickPerry\", \"Rick Perry\", text)\n",
    "    text = re.sub(r\"frontpage\", \"front page\", text)\n",
    "    text = re.sub(r\"NewsIntexts\", \"News In texts\", text)\n",
    "    text = re.sub(r\"ViralSpell\", \"Viral Spell\", text)\n",
    "    text = re.sub(r\"til_now\", \"until now\", text)\n",
    "    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n",
    "    text = re.sub(r\"ZippedNews\", \"Zipped News\", text)\n",
    "    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\", text)\n",
    "    text = re.sub(r\"53inch\", \"53 inch\", text)\n",
    "    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"Beyhive\", \"Beyonce hive\", text)\n",
    "    text = re.sub(r\"IDFire\", \"Idaho Fire\", text)\n",
    "    text = re.sub(r\"DETECTADO\", \"Detected\", text)\n",
    "    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n",
    "    text = re.sub(r\"Listen/Buy\", \"Listen / Buy\", text)\n",
    "    text = re.sub(r\"NickCannon\", \"Nick Cannon\", text)\n",
    "    text = re.sub(r\"FaroeIslands\", \"Faroe Islands\", text)\n",
    "    text = re.sub(r\"yycstorm\", \"Calgary Storm\", text)\n",
    "    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\", text)\n",
    "    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n",
    "    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text)\n",
    "    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\", text)\n",
    "    text = re.sub(r\"justinbieber\", \"justin bieber\", text)  \n",
    "    text = re.sub(r\"UTC2015\", \"UTC 2015\", text)\n",
    "    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n",
    "    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n",
    "    text = re.sub(r\"LivingSafely\", \"Living Safely\", text)\n",
    "    text = re.sub(r\"FIFA16\", \"Fifa 2016\", text)\n",
    "    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n",
    "    text = re.sub(r\"bbcnews\", \"bbc news\", text)\n",
    "    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text)\n",
    "    text = re.sub(r\"c4news\", \"c4 news\", text)\n",
    "    text = re.sub(r\"OBLITERATION\", \"obliteration\", text)\n",
    "    text = re.sub(r\"MUDSLIDE\", \"mudslide\", text)\n",
    "    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n",
    "    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"LondonFire\", \"London Fire\", text)\n",
    "    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\", text)\n",
    "    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text)\n",
    "    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\", text)\n",
    "    text = re.sub(r\"LiveOnK2\", \"Live On K2\", text)\n",
    "    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text)\n",
    "    text = re.sub(r\"nikeplus\", \"nike plus\", text)\n",
    "    text = re.sub(r\"david_cameron\", \"David Cameron\", text)\n",
    "    text = re.sub(r\"peterjukes\", \"Peter Jukes\", text)\n",
    "    text = re.sub(r\"JamesMelville\", \"James Melville\", text)\n",
    "    text = re.sub(r\"megynkelly\", \"Megyn Kelly\", text)\n",
    "    text = re.sub(r\"cnewslive\", \"C News Live\", text)\n",
    "    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text)\n",
    "    text = re.sub(r\"textLikeItsSeptember11th2001\", \"text like it is september 11th 2001\", text)\n",
    "    text = re.sub(r\"cbplawyers\", \"cbp lawyers\", text)\n",
    "    text = re.sub(r\"fewmoretexts\", \"few more texts\", text)\n",
    "    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n",
    "    text = re.sub(r\"cjoyner\", \"Chris Joyner\", text)\n",
    "    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n",
    "    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n",
    "    text = re.sub(r\"MikeParrActor\", \"Michael Parr\", text)\n",
    "    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text)\n",
    "    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text)\n",
    "    text = re.sub(r\"realmandyrain\", \"Mandy Rain\", text)\n",
    "    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text)\n",
    "    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\", text)\n",
    "    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n",
    "    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text)\n",
    "    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\", text)\n",
    "    text = re.sub(r\"ShaunKing\", \"Shaun King\", text)\n",
    "    text = re.sub(r\"MeekMill\", \"Meek Mill\", text)\n",
    "    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n",
    "    text = re.sub(r\"GRupdates\", \"GR updates\", text)\n",
    "    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n",
    "    text = re.sub(r\"braininjury\", \"brain injury\", text)\n",
    "    text = re.sub(r\"auspol\", \"Australian politics\", text)\n",
    "    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text)\n",
    "    text = re.sub(r\"calgaryweather\", \"Calgary Weather\", text)\n",
    "    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text)\n",
    "    text = re.sub(r\"edsheeran\", \"Ed Sheeran\", text)\n",
    "    text = re.sub(r\"TrueHeroes\", \"True Heroes\", text)\n",
    "    text = re.sub(r\"S3XLEAK\", \"sex leak\", text)\n",
    "    text = re.sub(r\"ComplexMag\", \"Complex Magazine\", text)\n",
    "    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text)\n",
    "    text = re.sub(r\"CityofCalgary\", \"City of Calgary\", text)\n",
    "    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text)\n",
    "    text = re.sub(r\"SummerFate\", \"Summer Fate\", text)\n",
    "    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n",
    "    text = re.sub(r\"offers2go\", \"offers to go\", text)\n",
    "    text = re.sub(r\"foodscare\", \"food scare\", text)\n",
    "    text = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", text)\n",
    "    text = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", text)\n",
    "    text = re.sub(r\"GamerGate\", \"Gamer Gate\", text)\n",
    "    text = re.sub(r\"IHHen\", \"Humanitarian Relief\", text)\n",
    "    text = re.sub(r\"spinningbot\", \"spinning bot\", text)\n",
    "    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\", text)\n",
    "    text = re.sub(r\"TAXIWAYS\", \"taxi ways\", text)\n",
    "    text = re.sub(r\"Calum5SOS\", \"Calum Hood\", text)\n",
    "    text = re.sub(r\"po_st\", \"po.st\", text)\n",
    "    text = re.sub(r\"scoopit\", \"scoop.it\", text)\n",
    "    text = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", text)\n",
    "    text = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", text)\n",
    "    text = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", text)\n",
    "    text = re.sub(r\"rapidcity\", \"Rapid City\", text)\n",
    "    text = re.sub(r\"OutBid\", \"outbid\", text)\n",
    "    text = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", text)\n",
    "    text = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", text)\n",
    "    text = re.sub(r\"15PM\", \"15 PM\", text)\n",
    "    text = re.sub(r\"OriginalFunko\", \"Funko\", text)\n",
    "    text = re.sub(r\"rightwaystan\", \"Richard Tan\", text)\n",
    "    text = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", text)\n",
    "    text = re.sub(r\"RT_America\", \"RT America\", text)\n",
    "    text = re.sub(r\"narendramodi\", \"Narendra Modi\", text)\n",
    "    text = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", text)\n",
    "    text = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", text)\n",
    "    text = re.sub(r\"alexbelloli\", \"Alex Belloli\", text)\n",
    "    text = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", text)\n",
    "    text = re.sub(r\"gunsense\", \"gun sense\", text)\n",
    "    text = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", text)\n",
    "    text = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", text)\n",
    "    text = re.sub(r\"samanthaturne19\", \"Samantha Turner\", text)\n",
    "    text = re.sub(r\"JonVoyage\", \"Jon Stewart\", text)\n",
    "    text = re.sub(r\"renew911health\", \"renew 911 health\", text)\n",
    "    text = re.sub(r\"SuryaRay\", \"Surya Ray\", text)\n",
    "    text = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", text)\n",
    "    text = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", text)\n",
    "    text = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", text)\n",
    "    text = re.sub(r\"pmarca\", \"Marc Andreessen\", text)\n",
    "    text = re.sub(r\"pdx911\", \"Portland Police\", text)\n",
    "    text = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", text)\n",
    "    text = re.sub(r\"Japton\", \"Arkansas\", text)\n",
    "    text = re.sub(r\"RouteComplex\", \"Route Complex\", text)\n",
    "    text = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", text)\n",
    "    text = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", text)\n",
    "    text = re.sub(r\"Politifiact\", \"PolitiFact\", text)\n",
    "    text = re.sub(r\"Hiroshima70\", \"Hiroshima\", text)\n",
    "    text = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", text)\n",
    "    text = re.sub(r\"versethe\", \"verse the\", text)\n",
    "    text = re.sub(r\"TubeStrike\", \"Tube Strike\", text)\n",
    "    text = re.sub(r\"MissionHills\", \"Mission Hills\", text)\n",
    "    text = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", text)\n",
    "    text = re.sub(r\"NANKANA\", \"Nankana\", text)\n",
    "    text = re.sub(r\"SAHIB\", \"Sahib\", text)\n",
    "    text = re.sub(r\"PAKPATTAN\", \"Pakpattan\", text)\n",
    "    text = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", text)\n",
    "    text = re.sub(r\"gofundme\", \"go fund me\", text)\n",
    "    text = re.sub(r\"pmharper\", \"Stephen Harper\", text)\n",
    "    text = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", text)\n",
    "    text = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", text)\n",
    "    text = re.sub(r\"bancodeseries\", \"banco de series\", text)\n",
    "    text = re.sub(r\"timkaine\", \"Tim Kaine\", text)\n",
    "    text = re.sub(r\"IdentityTheft\", \"Identity Theft\", text)\n",
    "    text = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", text)\n",
    "    text = re.sub(r\"mishacollins\", \"Misha Collins\", text)\n",
    "    text = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", text)\n",
    "    text = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", text)\n",
    "    text = re.sub(r\"Kowing\", \"Knowing\", text)\n",
    "    text = re.sub(r\"ScreamQueens\", \"Scream Queens\", text)\n",
    "    text = re.sub(r\"AskCharley\", \"Ask Charley\", text)\n",
    "    \n",
    "    # Urls\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n",
    "    text = re.sub(r\"mÌ¼sica\", \"music\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)    \n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n",
    "    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)   \n",
    "    \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 1.54 s, total: 3min 59s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['text'] = data['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigns the 'text' column to the variable X_data and the 'target' column to the variable y_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['target'] == 4, 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            switchfoot http      twitpic  com      awww ...\n",
       "1          upset cannot updat facebook text       might c...\n",
       "2            kenichan dive mani time ball   manag save   ...\n",
       "3                           whole bodi feel itchi like fire \n",
       "4            nationwideclass behav   i mad     cannot see   \n",
       "                                 ...                        \n",
       "1599995                        woke   school best feel ever \n",
       "1599996    thewdb  com    cool hear old walt interview   ...\n",
       "1599997                      readi mojo makeov   ask detail \n",
       "1599998    happi  birthday boo alll time       tupac amar...\n",
       "1599999    happi   charitytuesday   thenspcc   sparkschar...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = data['text']\n",
    "y_data = data['target']\n",
    "\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_truncated, X_discarded, y_truncated, y_discarded = train_test_split(X_data, y_data, train_size=0.08, stratify=y_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000 1200000\n",
      "400000 400000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000 96000\n",
      "32000 32000\n"
     ]
    }
   ],
   "source": [
    "x_train_trunc, x_test_trunc, y_train_trunc, y_test_trunc = train_test_split(X_truncated, y_truncated, random_state=42)\n",
    "print(len(x_train_trunc), len(y_train_trunc))\n",
    "print(len(x_test_trunc), len(y_test_trunc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer and Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer** operates on each individual text and performs the following steps:\n",
    "\n",
    "* **Tokenization**: It splits each document into individual words or terms, which are also referred to as tokens. \n",
    "* **Counting**: It counts the occurrence of each token in each document and creates a matrix where columns represent unique tokens.\n",
    "* **Vectorization**: It assigns a numerical value (count) to each token in each document, indicating how many times the token appears in that document.\n",
    "* **Vocabulary** **Creation**: It builds a vocabulary of unique tokens based on the training data. Each token corresponds to a specific column in the matrix.\n",
    "* **Transforming** **Test** **Data**: When applied to test data, the CountVectorizer uses the learned vocabulary from the training data and creates the matrix of token counts using the same columns as in the training matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** are a set of commonly used words in a language that are considered insignificant and are often removed during text preprocessing or natural language processing tasks. These words are filtered out because they typically do not carry much meaning or contribute significantly to the overall understanding of the text. Examples of stop words in English include \"the,\" \"is,\" \"and,\" \"a,\" \"an,\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "x_train_vectorizer=count.fit_transform(x_train)\n",
    "x_test_vectorizer=count.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nathyeah': 9468685,\n",
       " 'dame': 2891228,\n",
       " 'nathyeah dame': 9468686,\n",
       " 'boburnham': 1438620,\n",
       " 'thesaurus': 13861185,\n",
       " 'world': 15995103,\n",
       " 'sale': 11942029,\n",
       " 'end': 3829507,\n",
       " 'boburnham thesaurus': 1438646,\n",
       " 'thesaurus world': 13861202,\n",
       " 'world sale': 16003462,\n",
       " 'sale end': 11942671,\n",
       " 'boburnham thesaurus world': 1438647,\n",
       " 'thesaurus world sale': 13861203,\n",
       " 'world sale end': 16003463,\n",
       " 'boburnham thesaurus world sale': 1438648,\n",
       " 'thesaurus world sale end': 13861204,\n",
       " 'boburnham thesaurus world sale end': 1438649,\n",
       " 'jojoalexand': 7057980,\n",
       " 'ight': 6554051,\n",
       " 'let': 7697576,\n",
       " 'lil': 7890659,\n",
       " 'white': 15677926,\n",
       " 'boy': 1549032,\n",
       " 'know': 7355440,\n",
       " 'hahaha': 5632379,\n",
       " 'jojoalexand ight': 7057981,\n",
       " 'ight let': 6554101,\n",
       " 'let lil': 7710510,\n",
       " 'lil white': 7896275,\n",
       " 'white boy': 15678337,\n",
       " 'boy know': 1554319,\n",
       " 'know hahaha': 7379516,\n",
       " 'jojoalexand ight let': 7057982,\n",
       " 'ight let lil': 6554102,\n",
       " 'let lil white': 7710517,\n",
       " 'lil white boy': 7896276,\n",
       " 'white boy know': 15678345,\n",
       " 'boy know hahaha': 1554332,\n",
       " 'jojoalexand ight let lil': 7057983,\n",
       " 'ight let lil white': 6554103,\n",
       " 'let lil white boy': 7710518,\n",
       " 'lil white boy know': 7896277,\n",
       " 'white boy know hahaha': 15678346,\n",
       " 'jojoalexand ight let lil white': 7057984,\n",
       " 'ight let lil white boy': 6554104,\n",
       " 'let lil white boy know': 7710519,\n",
       " 'lil white boy know hahaha': 7896278,\n",
       " 'tweetlat': 14723694,\n",
       " 'pro': 10962165,\n",
       " 'way': 15459706,\n",
       " 'realli': 11393696,\n",
       " 'want': 15280977,\n",
       " 'work': 15897189,\n",
       " 'like': 7776165,\n",
       " 'http': 6387026,\n",
       " 'pinurl': 10640926,\n",
       " 'com': 2375068,\n",
       " 'ukw': 14876652,\n",
       " 'tweetlat pro': 14723716,\n",
       " 'pro way': 10963890,\n",
       " 'way realli': 15482754,\n",
       " 'realli want': 11452931,\n",
       " 'want work': 15351984,\n",
       " 'work like': 15936831,\n",
       " 'like http': 7821114,\n",
       " 'http pinurl': 6403076,\n",
       " 'pinurl com': 10640927,\n",
       " 'com ukw': 2419733,\n",
       " 'tweetlat pro way': 14723717,\n",
       " 'pro way realli': 10963891,\n",
       " 'way realli want': 15482833,\n",
       " 'realli want work': 11457211,\n",
       " 'want work like': 15352302,\n",
       " 'work like http': 15937072,\n",
       " 'like http pinurl': 7821207,\n",
       " 'http pinurl com': 6403077,\n",
       " 'pinurl com ukw': 10640931,\n",
       " 'tweetlat pro way realli': 14723718,\n",
       " 'pro way realli want': 10963892,\n",
       " 'way realli want work': 15482842,\n",
       " 'realli want work like': 11457243,\n",
       " 'want work like http': 15352305,\n",
       " 'work like http pinurl': 15937073,\n",
       " 'like http pinurl com': 7821208,\n",
       " 'http pinurl com ukw': 6403080,\n",
       " 'tweetlat pro way realli want': 14723719,\n",
       " 'pro way realli want work': 10963893,\n",
       " 'way realli want work like': 15482843,\n",
       " 'realli want work like http': 11457244,\n",
       " 'want work like http pinurl': 15352306,\n",
       " 'work like http pinurl com': 15937074,\n",
       " 'like http pinurl com ukw': 7821209,\n",
       " 'dudendaeaseonup': 3623172,\n",
       " 'love': 8277868,\n",
       " 'dudendaeaseonup love': 3623173,\n",
       " 'repeat': 11578410,\n",
       " 'ipod': 6801451,\n",
       " 'need': 9498470,\n",
       " 'make': 8526296,\n",
       " 'comeback': 2472308,\n",
       " 'bit': 1308813,\n",
       " 'ly': 8450002,\n",
       " 'stgtu': 13165958,\n",
       " 'repeat ipod': 11578860,\n",
       " 'ipod need': 6803999,\n",
       " 'need make': 9527401,\n",
       " 'make comeback': 8533407,\n",
       " 'comeback http': 2472350,\n",
       " 'http bit': 6387739,\n",
       " 'bit ly': 1317665,\n",
       " 'ly stgtu': 8460219,\n",
       " 'repeat ipod need': 11578861,\n",
       " 'ipod need make': 6804019,\n",
       " 'need make comeback': 9527494,\n",
       " 'make comeback http': 8533410,\n",
       " 'comeback http bit': 2472351,\n",
       " 'http bit ly': 6387754,\n",
       " 'bit ly stgtu': 1325785,\n",
       " 'repeat ipod need make': 11578862,\n",
       " 'ipod need make comeback': 6804020,\n",
       " 'need make comeback http': 9527495,\n",
       " 'make comeback http bit': 8533411,\n",
       " 'comeback http bit ly': 2472352,\n",
       " 'http bit ly stgtu': 6393673,\n",
       " 'repeat ipod need make comeback': 11578863,\n",
       " 'ipod need make comeback http': 6804021,\n",
       " 'need make comeback http bit': 9527496,\n",
       " 'make comeback http bit ly': 8533412,\n",
       " 'comeback http bit ly stgtu': 2472353,\n",
       " 'haha': 5589358,\n",
       " 'miss': 9014468,\n",
       " 'myy': 9422013,\n",
       " 'mattthew': 8736208,\n",
       " 'haha miss': 5609598,\n",
       " 'miss myy': 9052438,\n",
       " 'myy mattthew': 9422181,\n",
       " 'haha miss myy': 5609694,\n",
       " 'miss myy mattthew': 9052445,\n",
       " 'haha miss myy mattthew': 5609695,\n",
       " 'gmorn': 5027040,\n",
       " 'ooh': 10162568,\n",
       " 'giirll': 4952993,\n",
       " 'monday': 9171556,\n",
       " 'gmorn ooh': 5027124,\n",
       " 'ooh giirll': 10163505,\n",
       " 'giirll monday': 4952994,\n",
       " 'gmorn ooh giirll': 5027125,\n",
       " 'ooh giirll monday': 10163506,\n",
       " 'gmorn ooh giirll monday': 5027126,\n",
       " 'year': 16247959,\n",
       " 'todayi': 14306461,\n",
       " 'school': 12099298,\n",
       " 'year todayi': 16265694,\n",
       " 'todayi school': 14306686,\n",
       " 'year todayi school': 16265695,\n",
       " 'home': 6150414,\n",
       " 'summer': 13357317,\n",
       " 'prettyy': 10941475,\n",
       " 'home summer': 6190424,\n",
       " 'summer prettyy': 13365617,\n",
       " 'home summer prettyy': 6190474,\n",
       " 'clingermangw': 2281948,\n",
       " 'thank': 13746506,\n",
       " 'lot': 8247070,\n",
       " 'hell': 5947068,\n",
       " 'compil': 2499676,\n",
       " 'upload': 14953240,\n",
       " 'crap': 2705470,\n",
       " 'wait': 15182960,\n",
       " 'till': 14080872,\n",
       " 'alpha': 312956,\n",
       " 'clingermangw thank': 2281949,\n",
       " 'thank lot': 13783725,\n",
       " 'lot like': 8256275,\n",
       " 'like hell': 7817898,\n",
       " 'hell compil': 5948049,\n",
       " 'compil upload': 2499936,\n",
       " 'upload crap': 14953767,\n",
       " 'crap wait': 2711383,\n",
       " 'wait till': 15216805,\n",
       " 'till alpha': 14080990,\n",
       " 'clingermangw thank lot': 2281950,\n",
       " 'thank lot like': 13783942,\n",
       " 'lot like hell': 8256337,\n",
       " 'like hell compil': 7817928,\n",
       " 'hell compil upload': 5948050,\n",
       " 'compil upload crap': 2499937,\n",
       " 'upload crap wait': 14953768,\n",
       " 'crap wait till': 2711410,\n",
       " 'wait till alpha': 15216835,\n",
       " 'clingermangw thank lot like': 2281951,\n",
       " 'thank lot like hell': 13783943,\n",
       " 'lot like hell compil': 8256338,\n",
       " 'like hell compil upload': 7817929,\n",
       " 'hell compil upload crap': 5948051,\n",
       " 'compil upload crap wait': 2499938,\n",
       " 'upload crap wait till': 14953769,\n",
       " 'crap wait till alpha': 2711411,\n",
       " 'clingermangw thank lot like hell': 2281952,\n",
       " 'thank lot like hell compil': 13783944,\n",
       " 'lot like hell compil upload': 8256339,\n",
       " 'like hell compil upload crap': 7817930,\n",
       " 'hell compil upload crap wait': 5948052,\n",
       " 'compil upload crap wait till': 2499939,\n",
       " 'upload crap wait till alpha': 14953770,\n",
       " 'hey': 6001008,\n",
       " 'infork': 6718457,\n",
       " 'fam': 4156977,\n",
       " 'pleas': 10730643,\n",
       " 'send': 12223260,\n",
       " 'best': 1170411,\n",
       " 'wish': 15753394,\n",
       " 'vladofromania': 15144576,\n",
       " 'stomach': 13182054,\n",
       " 'flu': 4507173,\n",
       " 'bad': 896663,\n",
       " 'hey infork': 6016420,\n",
       " 'infork fam': 6718458,\n",
       " 'fam pleas': 4158675,\n",
       " 'pleas send': 10743475,\n",
       " 'send best': 12223805,\n",
       " 'best wish': 1191090,\n",
       " 'wish vladofromania': 15795654,\n",
       " 'vladofromania stomach': 15144577,\n",
       " 'stomach flu': 13183081,\n",
       " 'flu realli': 4509625,\n",
       " 'realli bad': 11396093,\n",
       " 'hey infork fam': 6016421,\n",
       " 'infork fam pleas': 6718459,\n",
       " 'fam pleas send': 4158679,\n",
       " 'pleas send best': 10743487,\n",
       " 'send best wish': 12223826,\n",
       " 'best wish vladofromania': 1191280,\n",
       " 'wish vladofromania stomach': 15795655,\n",
       " 'vladofromania stomach flu': 15144578,\n",
       " 'stomach flu realli': 13183135,\n",
       " 'flu realli bad': 4509626,\n",
       " 'hey infork fam pleas': 6016422,\n",
       " 'infork fam pleas send': 6718460,\n",
       " 'fam pleas send best': 4158680,\n",
       " 'pleas send best wish': 10743493,\n",
       " 'send best wish vladofromania': 12223830,\n",
       " 'best wish vladofromania stomach': 1191281,\n",
       " 'wish vladofromania stomach flu': 15795656,\n",
       " 'vladofromania stomach flu realli': 15144579,\n",
       " 'stomach flu realli bad': 13183136,\n",
       " 'hey infork fam pleas send': 6016423,\n",
       " 'infork fam pleas send best': 6718461,\n",
       " 'fam pleas send best wish': 4158681,\n",
       " 'pleas send best wish vladofromania': 10743494,\n",
       " 'send best wish vladofromania stomach': 12223831,\n",
       " 'best wish vladofromania stomach flu': 1191282,\n",
       " 'wish vladofromania stomach flu realli': 15795657,\n",
       " 'vladofromania stomach flu realli bad': 15144580,\n",
       " 'figpybfo': 4363626,\n",
       " 'ahah': 193199,\n",
       " 'mum': 9368673,\n",
       " 'figpybfo ahah': 4363627,\n",
       " 'ahah mum': 193565,\n",
       " 'figpybfo ahah mum': 4363628,\n",
       " 'techno': 13661223,\n",
       " 'parti': 10357302,\n",
       " 'took': 14421237,\n",
       " 'place': 10661380,\n",
       " 'room': 11782219,\n",
       " 'night': 9725286,\n",
       " 'complet': 2501863,\n",
       " 'destroy': 3258320,\n",
       " 'sleep': 12609428,\n",
       " 'techno parti': 13661345,\n",
       " 'parti took': 10370235,\n",
       " 'took place': 14428764,\n",
       " 'place room': 10668833,\n",
       " 'room night': 11787683,\n",
       " 'night complet': 9732326,\n",
       " 'complet destroy': 2503229,\n",
       " 'destroy sleep': 3259027,\n",
       " 'techno parti took': 13661349,\n",
       " 'parti took place': 10370239,\n",
       " 'took place room': 14428787,\n",
       " 'place room night': 10668838,\n",
       " 'room night complet': 11787690,\n",
       " 'night complet destroy': 9732327,\n",
       " 'complet destroy sleep': 2503248,\n",
       " 'techno parti took place': 13661350,\n",
       " 'parti took place room': 10370240,\n",
       " 'took place room night': 14428788,\n",
       " 'place room night complet': 10668839,\n",
       " 'room night complet destroy': 11787691,\n",
       " 'night complet destroy sleep': 9732328,\n",
       " 'techno parti took place room': 13661351,\n",
       " 'parti took place room night': 10370241,\n",
       " 'took place room night complet': 14428789,\n",
       " 'place room night complet destroy': 10668840,\n",
       " 'room night complet destroy sleep': 11787692,\n",
       " 'wrong': 16077438,\n",
       " 'thing': 13870626,\n",
       " 'noth': 9859632,\n",
       " 'stuff': 13278697,\n",
       " 'lame': 7509824,\n",
       " 'write': 16062582,\n",
       " 'affili': 142018,\n",
       " 'god': 5032845,\n",
       " 'help': 5970253,\n",
       " 'wrong thing': 16083712,\n",
       " 'thing noth': 13890743,\n",
       " 'noth stuff': 9870503,\n",
       " 'stuff lame': 13284219,\n",
       " 'lame write': 7512462,\n",
       " 'write affili': 16062694,\n",
       " 'affili god': 142081,\n",
       " 'god help': 5038891,\n",
       " 'wrong thing noth': 16083747,\n",
       " 'thing noth stuff': 13890765,\n",
       " 'noth stuff lame': 9870508,\n",
       " 'stuff lame write': 13284225,\n",
       " 'lame write affili': 7512463,\n",
       " 'write affili god': 16062695,\n",
       " 'affili god help': 142082,\n",
       " 'wrong thing noth stuff': 16083748,\n",
       " 'thing noth stuff lame': 13890766,\n",
       " 'noth stuff lame write': 9870509,\n",
       " 'stuff lame write affili': 13284226,\n",
       " 'lame write affili god': 7512464,\n",
       " 'write affili god help': 16062696,\n",
       " 'wrong thing noth stuff lame': 16083749,\n",
       " 'thing noth stuff lame write': 13890767,\n",
       " 'noth stuff lame write affili': 9870510,\n",
       " 'stuff lame write affili god': 13284227,\n",
       " 'lame write affili god help': 7512465,\n",
       " 'radiocolin': 11251721,\n",
       " 'laugh': 7567285,\n",
       " 'fli': 4480749,\n",
       " 'rochest': 11750707,\n",
       " 'ny': 9908275,\n",
       " 'luggag': 8421771,\n",
       " 'went': 15622318,\n",
       " 'manchest': 8626906,\n",
       " 'england': 3850960,\n",
       " 'radiocolin laugh': 11251726,\n",
       " 'laugh fli': 7573676,\n",
       " 'fli rochest': 4484250,\n",
       " 'rochest ny': 11750783,\n",
       " 'ny luggag': 9909464,\n",
       " 'luggag went': 8422084,\n",
       " 'went manchest': 15635038,\n",
       " 'manchest england': 8627116,\n",
       " 'radiocolin laugh fli': 11251727,\n",
       " 'laugh fli rochest': 7573677,\n",
       " 'fli rochest ny': 4484251,\n",
       " 'rochest ny luggag': 11750785,\n",
       " 'ny luggag went': 9909466,\n",
       " 'luggag went manchest': 8422086,\n",
       " 'went manchest england': 15635039,\n",
       " 'radiocolin laugh fli rochest': 11251728,\n",
       " 'laugh fli rochest ny': 7573678,\n",
       " 'fli rochest ny luggag': 4484252,\n",
       " 'rochest ny luggag went': 11750786,\n",
       " 'ny luggag went manchest': 9909467,\n",
       " 'luggag went manchest england': 8422087,\n",
       " 'radiocolin laugh fli rochest ny': 11251729,\n",
       " 'laugh fli rochest ny luggag': 7573679,\n",
       " 'fli rochest ny luggag went': 4484253,\n",
       " 'rochest ny luggag went manchest': 11750787,\n",
       " 'ny luggag went manchest england': 9909468,\n",
       " 'jotac': 7097422,\n",
       " 'thought': 14003445,\n",
       " 'pic': 10581290,\n",
       " 'look': 8154100,\n",
       " 'littl': 7967336,\n",
       " 'dramat': 3529240,\n",
       " 'think': 13905392,\n",
       " 'sad': 11875926,\n",
       " 'chang': 2041663,\n",
       " 'jotac thank': 7097423,\n",
       " 'thank thought': 13805122,\n",
       " 'thought pic': 14015884,\n",
       " 'pic look': 10587701,\n",
       " 'look littl': 8190334,\n",
       " 'littl dramat': 7975185,\n",
       " 'dramat think': 3529467,\n",
       " 'think look': 13944855,\n",
       " 'littl sad': 7986514,\n",
       " 'sad think': 11905346,\n",
       " 'think chang': 13915720,\n",
       " 'jotac thank thought': 7097424,\n",
       " 'thank thought pic': 13805228,\n",
       " 'thought pic look': 14015901,\n",
       " 'pic look littl': 10587808,\n",
       " 'look littl dramat': 8190378,\n",
       " 'littl dramat think': 7975187,\n",
       " 'dramat think look': 3529468,\n",
       " 'think look littl': 13945060,\n",
       " 'look littl sad': 8190425,\n",
       " 'littl sad think': 7986730,\n",
       " 'sad think chang': 11905370,\n",
       " 'jotac thank thought pic': 7097425,\n",
       " 'thank thought pic look': 13805229,\n",
       " 'thought pic look littl': 14015902,\n",
       " 'pic look littl dramat': 10587809,\n",
       " 'look littl dramat think': 8190379,\n",
       " 'littl dramat think look': 7975188,\n",
       " 'dramat think look littl': 3529469,\n",
       " 'think look littl sad': 13945061,\n",
       " 'look littl sad think': 8190429,\n",
       " 'littl sad think chang': 7986731,\n",
       " 'jotac thank thought pic look': 7097426,\n",
       " 'thank thought pic look littl': 13805230,\n",
       " 'thought pic look littl dramat': 14015903,\n",
       " 'pic look littl dramat think': 10587810,\n",
       " 'look littl dramat think look': 8190380,\n",
       " 'littl dramat think look littl': 7975189,\n",
       " 'dramat think look littl sad': 3529470,\n",
       " 'think look littl sad think': 13945062,\n",
       " 'look littl sad think chang': 8190430,\n",
       " 'sepinwal': 12246650,\n",
       " 'use': 14996734,\n",
       " 'plcl': 10730288,\n",
       " 'kubiac': 7460937,\n",
       " 'man': 8599158,\n",
       " 'cooler': 2614965,\n",
       " 'grew': 5460979,\n",
       " 'jerri': 6972735,\n",
       " 'er': 3902864,\n",
       " 'sepinwal use': 12246655,\n",
       " 'use love': 15009457,\n",
       " 'love plcl': 8344984,\n",
       " 'plcl kubiac': 10730289,\n",
       " 'kubiac man': 7460938,\n",
       " 'man cooler': 8602058,\n",
       " 'cooler grew': 2615168,\n",
       " 'grew chang': 5461066,\n",
       " 'chang jerri': 2045821,\n",
       " 'jerri er': 6972866,\n",
       " 'sepinwal use love': 12246656,\n",
       " 'use love plcl': 15009638,\n",
       " 'love plcl kubiac': 8344985,\n",
       " 'plcl kubiac man': 10730290,\n",
       " 'kubiac man cooler': 7460939,\n",
       " 'man cooler grew': 8602059,\n",
       " 'cooler grew chang': 2615169,\n",
       " 'grew chang jerri': 5461067,\n",
       " 'chang jerri er': 2045822,\n",
       " 'sepinwal use love plcl': 12246657,\n",
       " 'use love plcl kubiac': 15009639,\n",
       " 'love plcl kubiac man': 8344986,\n",
       " 'plcl kubiac man cooler': 10730291,\n",
       " 'kubiac man cooler grew': 7460940,\n",
       " 'man cooler grew chang': 8602060,\n",
       " 'cooler grew chang jerri': 2615170,\n",
       " 'grew chang jerri er': 5461068,\n",
       " 'sepinwal use love plcl kubiac': 12246658,\n",
       " 'use love plcl kubiac man': 15009640,\n",
       " 'love plcl kubiac man cooler': 8344987,\n",
       " 'plcl kubiac man cooler grew': 10730292,\n",
       " 'kubiac man cooler grew chang': 7460941,\n",
       " 'man cooler grew chang jerri': 8602061,\n",
       " 'cooler grew chang jerri er': 2615171,\n",
       " 'twitpic': 14744257,\n",
       " 'lotion': 8267915,\n",
       " 'wonder': 15845343,\n",
       " 'http twitpic': 6410451,\n",
       " 'twitpic com': 14744465,\n",
       " 'com lotion': 2399698,\n",
       " 'lotion wonder': 8268172,\n",
       " 'http twitpic com': 6410452,\n",
       " 'twitpic com lotion': 14755542,\n",
       " 'com lotion wonder': 2399699,\n",
       " 'http twitpic com lotion': 6417019,\n",
       " 'twitpic com lotion wonder': 14755543,\n",
       " 'http twitpic com lotion wonder': 6417020,\n",
       " 'dopegirlfresh': 3489750,\n",
       " 'titti': 14214286,\n",
       " 'grab': 5370305,\n",
       " 'feel': 4258084,\n",
       " 'better': 1205116,\n",
       " 'dopegirlfresh let': 3489766,\n",
       " 'let titti': 7718771,\n",
       " 'titti grab': 14214322,\n",
       " 'grab feel': 5370910,\n",
       " 'feel better': 4262512,\n",
       " 'dopegirlfresh let titti': 3489767,\n",
       " 'let titti grab': 7718775,\n",
       " 'titti grab feel': 14214323,\n",
       " 'grab feel better': 5370911,\n",
       " 'dopegirlfresh let titti grab': 3489768,\n",
       " 'let titti grab feel': 7718776,\n",
       " 'titti grab feel better': 14214324,\n",
       " 'dopegirlfresh let titti grab feel': 3489769,\n",
       " 'let titti grab feel better': 7718777,\n",
       " 'mistawilli': 9094527,\n",
       " 'happi': 5725511,\n",
       " 'bday': 1031189,\n",
       " 'hope': 6230202,\n",
       " 'good': 5118208,\n",
       " 'mistawilli happi': 9094530,\n",
       " 'happi bday': 5726740,\n",
       " 'bday hope': 1032605,\n",
       " 'hope good': 6249592,\n",
       " 'mistawilli happi bday': 9094531,\n",
       " 'happi bday hope': 5726943,\n",
       " 'bday hope good': 1032617,\n",
       " 'mistawilli happi bday hope': 9094532,\n",
       " 'happi bday hope good': 5726952,\n",
       " 'mistawilli happi bday hope good': 9094533,\n",
       " 'cop': 2619055,\n",
       " 'mr': 9322352,\n",
       " 'picker': 10603755,\n",
       " 'cop look': 2619557,\n",
       " 'look mr': 8192369,\n",
       " 'mr picker': 9325566,\n",
       " 'cop look mr': 2619559,\n",
       " 'look mr picker': 8192376,\n",
       " 'cop look mr picker': 2619560,\n",
       " 'prob': 10964149,\n",
       " 'nemosocean': 9569672,\n",
       " 'ur': 14965771,\n",
       " 'awesom': 792579,\n",
       " 'hoki': 6128558,\n",
       " 'poki': 10788114,\n",
       " 'danc': 2919216,\n",
       " 'prob nemosocean': 10966143,\n",
       " 'nemosocean ur': 9569677,\n",
       " 'ur awesom': 14966728,\n",
       " 'awesom hoki': 800890,\n",
       " 'hoki poki': 6128562,\n",
       " 'poki danc': 10788115,\n",
       " 'prob nemosocean ur': 10966144,\n",
       " 'nemosocean ur awesom': 9569678,\n",
       " 'ur awesom hoki': 14966745,\n",
       " 'awesom hoki poki': 800891,\n",
       " 'hoki poki danc': 6128563,\n",
       " 'prob nemosocean ur awesom': 10966145,\n",
       " 'nemosocean ur awesom hoki': 9569679,\n",
       " 'ur awesom hoki poki': 14966746,\n",
       " 'awesom hoki poki danc': 800892,\n",
       " 'prob nemosocean ur awesom hoki': 10966146,\n",
       " 'nemosocean ur awesom hoki poki': 9569680,\n",
       " 'ur awesom hoki poki danc': 14966747,\n",
       " 'lemongener': 7684901,\n",
       " 'hahahaha': 5643148,\n",
       " 'meal': 8783540,\n",
       " 'veggi': 15069335,\n",
       " 'edit': 3732965,\n",
       " 'lemongener hahahaha': 7684944,\n",
       " 'hahahaha happi': 5643811,\n",
       " 'happi meal': 5743195,\n",
       " 'meal veggi': 8784849,\n",
       " 'veggi edit': 15069519,\n",
       " 'lemongener hahahaha happi': 7684945,\n",
       " 'hahahaha happi meal': 5643814,\n",
       " 'happi meal veggi': 5743208,\n",
       " 'meal veggi edit': 8784850,\n",
       " 'lemongener hahahaha happi meal': 7684946,\n",
       " 'hahahaha happi meal veggi': 5643815,\n",
       " 'happi meal veggi edit': 5743209,\n",
       " 'lemongener hahahaha happi meal veggi': 7684947,\n",
       " 'hahahaha happi meal veggi edit': 5643816,\n",
       " 'ngebay': 9670319,\n",
       " 'yeahh': 16245414,\n",
       " 'cuz': 2839562,\n",
       " 'workin': 15989402,\n",
       " 'late': 7539020,\n",
       " 'nite': 9794626,\n",
       " 'ngebay yeahh': 9670320,\n",
       " 'yeahh cuz': 16245564,\n",
       " 'cuz workin': 2848602,\n",
       " 'workin till': 15990682,\n",
       " 'till late': 14085147,\n",
       " 'late nite': 7547688,\n",
       " 'ngebay yeahh cuz': 9670321,\n",
       " 'yeahh cuz workin': 16245565,\n",
       " 'cuz workin till': 2848603,\n",
       " 'workin till late': 15990696,\n",
       " 'till late nite': 14085186,\n",
       " 'ngebay yeahh cuz workin': 9670322,\n",
       " 'yeahh cuz workin till': 16245566,\n",
       " 'cuz workin till late': 2848604,\n",
       " 'workin till late nite': 15990697,\n",
       " 'ngebay yeahh cuz workin till': 9670323,\n",
       " 'yeahh cuz workin till late': 16245567,\n",
       " 'cuz workin till late nite': 2848605,\n",
       " 'mamajoan': 8597323,\n",
       " 'suck': 13320269,\n",
       " 'right': 11676302,\n",
       " 'mamajoan suck': 8597324,\n",
       " 'suck right': 13331467,\n",
       " 'mamajoan suck right': 8597325,\n",
       " 'base': 990258,\n",
       " 'train': 14506057,\n",
       " 'base train': 991887,\n",
       " 'deancoulson': 3141593,\n",
       " 'pain': 10296818,\n",
       " 'ah': 181716,\n",
       " 'trick': 14590233,\n",
       " 'deancoulson know': 3141606,\n",
       " 'know pain': 7400162,\n",
       " 'pain ah': 10296892,\n",
       " 'ah trick': 190147,\n",
       " 'trick train': 14590997,\n",
       " 'train home': 14508523,\n",
       " 'deancoulson know pain': 3141610,\n",
       " 'know pain ah': 7400163,\n",
       " 'pain ah trick': 10296896,\n",
       " 'ah trick train': 190148,\n",
       " 'trick train home': 14590998,\n",
       " 'deancoulson know pain ah': 3141611,\n",
       " 'know pain ah trick': 7400164,\n",
       " 'pain ah trick train': 10296897,\n",
       " 'ah trick train home': 190149,\n",
       " 'deancoulson know pain ah trick': 3141612,\n",
       " 'know pain ah trick train': 7400165,\n",
       " 'pain ah trick train home': 10296898,\n",
       " 'fun': 4774253,\n",
       " 'dri': 3556279,\n",
       " 'socket': 12735052,\n",
       " 'wisdom': 15751265,\n",
       " 'pull': 11050572,\n",
       " 'guess': 5497039,\n",
       " 'mean': 8785042,\n",
       " 'med': 8811036,\n",
       " 'fun fun': 4782338,\n",
       " 'fun dri': 4780070,\n",
       " 'dri socket': 3557932,\n",
       " 'socket wisdom': 12735128,\n",
       " 'wisdom pull': 15751494,\n",
       " 'pull guess': 11051697,\n",
       " 'guess mean': 5506179,\n",
       " 'mean pain': 8795622,\n",
       " 'pain med': 10300582,\n",
       " 'fun fun dri': 4782386,\n",
       " 'fun dri socket': 4780071,\n",
       " 'dri socket wisdom': 3557938,\n",
       " 'socket wisdom pull': 12735129,\n",
       " 'wisdom pull guess': 15751495,\n",
       " 'pull guess mean': 11051703,\n",
       " 'guess mean pain': 5506356,\n",
       " 'mean pain med': 8795626,\n",
       " 'fun fun dri socket': 4782387,\n",
       " 'fun dri socket wisdom': 4780072,\n",
       " 'dri socket wisdom pull': 3557939,\n",
       " 'socket wisdom pull guess': 12735130,\n",
       " 'wisdom pull guess mean': 15751496,\n",
       " 'pull guess mean pain': 11051704,\n",
       " 'guess mean pain med': 5506357,\n",
       " 'fun fun dri socket wisdom': 4782388,\n",
       " 'fun dri socket wisdom pull': 4780073,\n",
       " 'dri socket wisdom pull guess': 3557940,\n",
       " 'socket wisdom pull guess mean': 12735131,\n",
       " 'wisdom pull guess mean pain': 15751497,\n",
       " 'pull guess mean pain med': 11051705,\n",
       " 'anitavlacho': 449499,\n",
       " 'mi': 8907770,\n",
       " 'wow': 16035076,\n",
       " 'great': 5402986,\n",
       " 'hear': 5880605,\n",
       " 'chanc': 2036161,\n",
       " 'outsid': 10240129,\n",
       " 'savor': 12015590,\n",
       " 'anitavlacho mi': 449504,\n",
       " 'mi wow': 8909559,\n",
       " 'wow great': 16041391,\n",
       " 'great hear': 5418074,\n",
       " 'hear hope': 5886403,\n",
       " 'hope chanc': 6236370,\n",
       " 'chanc outsid': 2038714,\n",
       " 'outsid savor': 10246458,\n",
       " 'anitavlacho mi wow': 449505,\n",
       " 'mi wow great': 8909560,\n",
       " 'wow great hear': 16041475,\n",
       " 'great hear hope': 5418158,\n",
       " 'hear hope chanc': 5886432,\n",
       " 'hope chanc outsid': 6236401,\n",
       " 'chanc outsid savor': 2038715,\n",
       " 'anitavlacho mi wow great': 449506,\n",
       " 'mi wow great hear': 8909561,\n",
       " 'wow great hear hope': 16041476,\n",
       " 'great hear hope chanc': 5418159,\n",
       " 'hear hope chanc outsid': 5886433,\n",
       " 'hope chanc outsid savor': 6236402,\n",
       " 'anitavlacho mi wow great hear': 449507,\n",
       " 'mi wow great hear hope': 8909562,\n",
       " 'wow great hear hope chanc': 16041477,\n",
       " 'great hear hope chanc outsid': 5418160,\n",
       " 'hear hope chanc outsid savor': 5886434,\n",
       " 'maritzamendoza': 8676523,\n",
       " 'order': 10203494,\n",
       " 'rainboot': 11277242,\n",
       " 'onlin': 10150935,\n",
       " 'lack': 7482268,\n",
       " 'store': 13205307,\n",
       " 'option': 10197401,\n",
       " 'com maritzamendoza': 2401302,\n",
       " 'maritzamendoza end': 8676524,\n",
       " 'end order': 3837864,\n",
       " 'order need': 10206846,\n",
       " 'need rainboot': 9537262,\n",
       " 'rainboot onlin': 11277251,\n",
       " 'onlin lack': 10153563,\n",
       " 'lack store': 7483864,\n",
       " 'store option': 13208892,\n",
       " 'twitpic com maritzamendoza': 14756369,\n",
       " 'com maritzamendoza end': 2401303,\n",
       " 'maritzamendoza end order': 8676525,\n",
       " 'end order need': 3837877,\n",
       " 'order need rainboot': 10206858,\n",
       " 'need rainboot onlin': 9537263,\n",
       " 'rainboot onlin lack': 11277252,\n",
       " 'onlin lack store': 10153564,\n",
       " 'lack store option': 7483865,\n",
       " 'http twitpic com maritzamendoza': 6417475,\n",
       " 'twitpic com maritzamendoza end': 14756370,\n",
       " 'com maritzamendoza end order': 2401304,\n",
       " 'maritzamendoza end order need': 8676526,\n",
       " 'end order need rainboot': 3837878,\n",
       " 'order need rainboot onlin': 10206859,\n",
       " 'need rainboot onlin lack': 9537264,\n",
       " 'rainboot onlin lack store': 11277253,\n",
       " 'onlin lack store option': 10153565,\n",
       " 'http twitpic com maritzamendoza end': 6417476,\n",
       " 'twitpic com maritzamendoza end order': 14756371,\n",
       " 'com maritzamendoza end order need': 2401305,\n",
       " 'maritzamendoza end order need rainboot': 8676527,\n",
       " 'end order need rainboot onlin': 3837879,\n",
       " 'order need rainboot onlin lack': 10206860,\n",
       " 'need rainboot onlin lack store': 9537265,\n",
       " 'rainboot onlin lack store option': 11277254,\n",
       " 'thisstarchild': 13989369,\n",
       " 'thisstarchild miss': 13989431,\n",
       " 'miss fun': 9034702,\n",
       " 'thisstarchild miss fun': 13989432,\n",
       " 'ride': 11666600,\n",
       " 'today': 14229459,\n",
       " 'sick': 12447301,\n",
       " 'lot fun': 8252582,\n",
       " 'fun ride': 4794123,\n",
       " 'ride today': 11672082,\n",
       " 'today feel': 14248452,\n",
       " 'feel sick': 4305838,\n",
       " 'lot fun ride': 8253156,\n",
       " 'fun ride today': 4794149,\n",
       " 'ride today feel': 11672099,\n",
       " 'today feel sick': 14248986,\n",
       " 'lot fun ride today': 8253157,\n",
       " 'fun ride today feel': 4794150,\n",
       " 'ride today feel sick': 11672100,\n",
       " 'lot fun ride today feel': 8253158,\n",
       " 'fun ride today feel sick': 4794151,\n",
       " 'danjit': 2941033,\n",
       " 'welcom': 15607739,\n",
       " 'borrow': 1510767,\n",
       " 'time': 14093584,\n",
       " 'masterton': 8711425,\n",
       " 'danjit welcom': 2941034,\n",
       " 'welcom borrow': 15608403,\n",
       " 'borrow time': 1511528,\n",
       " 'time masterton': 14130669,\n",
       " 'danjit welcom borrow': 2941035,\n",
       " 'welcom borrow time': 15608404,\n",
       " 'borrow time masterton': 1511529,\n",
       " 'danjit welcom borrow time': 2941036,\n",
       " 'welcom borrow time masterton': 15608405,\n",
       " 'danjit welcom borrow time masterton': 2941037,\n",
       " 'anoth': 469808,\n",
       " 'headach': 5866223,\n",
       " 'oc': 9928171,\n",
       " 'tomorrow': 14347607,\n",
       " 'anoth headach': 478986,\n",
       " 'headach night': 5869611,\n",
       " 'night sleep': 9763349,\n",
       " 'sleep oc': 12630823,\n",
       " 'oc tomorrow': 9928525,\n",
       " 'anoth headach night': 479004,\n",
       " 'headach night sleep': 5869639,\n",
       " 'night sleep oc': 9763861,\n",
       " 'sleep oc tomorrow': 12630824,\n",
       " 'anoth headach night sleep': 479005,\n",
       " 'headach night sleep oc': 5869640,\n",
       " 'night sleep oc tomorrow': 9763862,\n",
       " 'anoth headach night sleep oc': 479006,\n",
       " 'headach night sleep oc tomorrow': 5869641,\n",
       " 'someth': 12773980,\n",
       " 'bb': 1015257,\n",
       " 'someth wrong': 12791396,\n",
       " 'wrong bb': 16077842,\n",
       " 'someth wrong bb': 12791435,\n",
       " 'tommcfli': 14336123,\n",
       " 'amaz': 370842,\n",
       " 'concert': 2523791,\n",
       " 'chile': 2148410,\n",
       " 'tommcfli awesom': 14336475,\n",
       " 'awesom amaz': 793098,\n",
       " 'amaz love': 378979,\n",
       " 'love concert': 8293057,\n",
       " 'concert hope': 2526020,\n",
       " 'hope like': 6257843,\n",
       " 'like chile': 7790485,\n",
       " 'tommcfli awesom amaz': 14336476,\n",
       " 'awesom amaz love': 793139,\n",
       " 'amaz love concert': 379056,\n",
       " 'love concert hope': 8293071,\n",
       " 'concert hope like': 2526042,\n",
       " 'hope like chile': 6257905,\n",
       " 'tommcfli awesom amaz love': 14336477,\n",
       " 'awesom amaz love concert': 793140,\n",
       " 'amaz love concert hope': 379059,\n",
       " 'love concert hope like': 8293072,\n",
       " 'concert hope like chile': 2526043,\n",
       " 'tommcfli awesom amaz love concert': 14336478,\n",
       " 'awesom amaz love concert hope': 793141,\n",
       " 'amaz love concert hope like': 379060,\n",
       " 'love concert hope like chile': 8293073,\n",
       " 'charlielevin': 2069259,\n",
       " 'root': 11793525,\n",
       " 'ya': 16146464,\n",
       " 'kick': 7247942,\n",
       " 'ass': 666694,\n",
       " 'charlielevin root': 2069260,\n",
       " 'root ya': 11794651,\n",
       " 'ya kick': 16151881,\n",
       " 'kick ass': 7248115,\n",
       " 'charlielevin root ya': 2069261,\n",
       " 'root ya kick': 11794653,\n",
       " 'ya kick ass': 16151882,\n",
       " 'charlielevin root ya kick': 2069262,\n",
       " 'root ya kick ass': 11794654,\n",
       " 'charlielevin root ya kick ass': 2069263,\n",
       " 'vivalatrac': 15142229,\n",
       " 'oh': 9963827,\n",
       " 'piss': 10643851,\n",
       " 'vivalatrac oh': 15142237,\n",
       " 'oh know': 9991335,\n",
       " 'know piss': 7401679,\n",
       " 'vivalatrac oh know': 15142238,\n",
       " 'oh know piss': 9991725,\n",
       " 'vivalatrac oh know piss': 15142239,\n",
       " 'say': 12036376,\n",
       " 'morn': 9225934,\n",
       " 'plurk': 10759023,\n",
       " 'say good': 12046317,\n",
       " 'good morn': 5164064,\n",
       " 'morn http': 9243419,\n",
       " 'http plurk': 6403131,\n",
       " 'plurk com': 10759113,\n",
       " 'say good morn': 12046584,\n",
       " 'good morn http': 5167957,\n",
       " 'morn http plurk': 9243446,\n",
       " 'http plurk com': 6403132,\n",
       " 'say good morn http': 12046643,\n",
       " 'good morn http plurk': 5167965,\n",
       " 'morn http plurk com': 9243447,\n",
       " 'say good morn http plurk': 12046644,\n",
       " 'good morn http plurk com': 5167966,\n",
       " 'new': 9590258,\n",
       " 'drug': 3598192,\n",
       " 'choci': 2171233,\n",
       " 'gnosischocol': 5029789,\n",
       " 'squarespac': 13039278,\n",
       " 'new drug': 9604309,\n",
       " 'drug choci': 3598323,\n",
       " 'choci http': 2171236,\n",
       " 'http gnosischocol': 6399476,\n",
       " 'gnosischocol squarespac': 5029792,\n",
       " 'squarespac com': 13039382,\n",
       " 'new drug choci': 9604310,\n",
       " 'drug choci http': 3598324,\n",
       " 'choci http gnosischocol': 2171237,\n",
       " 'http gnosischocol squarespac': 6399477,\n",
       " 'gnosischocol squarespac com': 5029793,\n",
       " 'new drug choci http': 9604311,\n",
       " 'drug choci http gnosischocol': 3598325,\n",
       " 'choci http gnosischocol squarespac': 2171238,\n",
       " 'http gnosischocol squarespac com': 6399478,\n",
       " 'new drug choci http gnosischocol': 9604312,\n",
       " 'drug choci http gnosischocol squarespac': 3598326,\n",
       " 'choci http gnosischocol squarespac com': 2171239,\n",
       " 'fragilebubbl': 4635168,\n",
       " 'thanx': 13815398,\n",
       " 'hun': 6459436,\n",
       " 'yeah': 16206771,\n",
       " 'real': 11369556,\n",
       " 'encourag': 3828732,\n",
       " 'yesterday': 16318285,\n",
       " 'financ': 4409776,\n",
       " 'screw': 12149185,\n",
       " 'hubbi': 6432624,\n",
       " 'wage': 15179431,\n",
       " 'fragilebubbl thanx': 4635189,\n",
       " 'thanx hun': 13816207,\n",
       " 'hun yeah': 6461827,\n",
       " 'yeah know': 16222952,\n",
       " 'know hear': 7381009,\n",
       " 'hear real': 5891148,\n",
       " 'real encourag': 11371657,\n",
       " 'encourag yesterday': 3829393,\n",
       " 'yesterday financ': 16322023,\n",
       " 'financ work': 4410124,\n",
       " 'work screw': 15958844,\n",
       " 'screw hubbi': 12150146,\n",
       " 'hubbi wage': 6436173,\n",
       " 'fragilebubbl thanx hun': 4635190,\n",
       " 'thanx hun yeah': 13816219,\n",
       " 'hun yeah know': 6461831,\n",
       " 'yeah know hear': 16223307,\n",
       " 'know hear real': 7381059,\n",
       " 'hear real encourag': 5891151,\n",
       " 'real encourag yesterday': 11371658,\n",
       " 'encourag yesterday financ': 3829394,\n",
       " 'yesterday financ work': 16322024,\n",
       " 'financ work screw': 4410128,\n",
       " 'work screw hubbi': 15958848,\n",
       " 'screw hubbi wage': 12150147,\n",
       " 'fragilebubbl thanx hun yeah': 4635191,\n",
       " 'thanx hun yeah know': 13816220,\n",
       " 'hun yeah know hear': 6461832,\n",
       " 'yeah know hear real': 16223308,\n",
       " 'know hear real encourag': 7381060,\n",
       " 'hear real encourag yesterday': 5891152,\n",
       " 'real encourag yesterday financ': 11371659,\n",
       " 'encourag yesterday financ work': 3829395,\n",
       " 'yesterday financ work screw': 16322025,\n",
       " 'financ work screw hubbi': 4410129,\n",
       " 'work screw hubbi wage': 15958849,\n",
       " 'fragilebubbl thanx hun yeah know': 4635192,\n",
       " 'thanx hun yeah know hear': 13816221,\n",
       " 'hun yeah know hear real': 6461833,\n",
       " 'yeah know hear real encourag': 16223309,\n",
       " 'know hear real encourag yesterday': 7381061,\n",
       " 'hear real encourag yesterday financ': 5891153,\n",
       " 'real encourag yesterday financ work': 11371660,\n",
       " 'encourag yesterday financ work screw': 3829396,\n",
       " 'yesterday financ work screw hubbi': 16322026,\n",
       " 'financ work screw hubbi wage': 4410130,\n",
       " 'tri': 14543518,\n",
       " 'wrap': 16057865,\n",
       " 'relax': 11535226,\n",
       " 'tri wrap': 14587597,\n",
       " 'wrap work': 16059147,\n",
       " 'work relax': 15955846,\n",
       " 'relax home': 11537119,\n",
       " 'tri wrap work': 14587606,\n",
       " 'wrap work relax': 16059157,\n",
       " 'work relax home': 15955860,\n",
       " 'tri wrap work relax': 14587607,\n",
       " 'wrap work relax home': 16059158,\n",
       " 'tri wrap work relax home': 14587608,\n",
       " 'day': 3001392,\n",
       " 'unyummi': 14936216,\n",
       " 'food': 4564272,\n",
       " 'wagon': 15179713,\n",
       " 'weather': 15505963,\n",
       " 'perfect': 10486853,\n",
       " 'bring': 1631879,\n",
       " 'day unyummi': 3099482,\n",
       " 'unyummi food': 14936217,\n",
       " 'food wagon': 4574174,\n",
       " 'wagon went': 15179859,\n",
       " 'went realli': 15638953,\n",
       " 'realli weather': 11457723,\n",
       " 'weather perfect': 15514274,\n",
       " 'perfect ride': 10490745,\n",
       " 'ride home': 11668972,\n",
       " 'home tomorrow': 6194020,\n",
       " 'tomorrow bring': 14350602,\n",
       " 'day unyummi food': 3099483,\n",
       " 'unyummi food wagon': 14936218,\n",
       " 'food wagon went': 4574175,\n",
       " 'wagon went realli': 15179860,\n",
       " 'went realli weather': 15639100,\n",
       " 'realli weather perfect': 11457730,\n",
       " 'weather perfect ride': 15514315,\n",
       " 'perfect ride home': 10490746,\n",
       " 'ride home tomorrow': 11669145,\n",
       " 'home tomorrow bring': 6194058,\n",
       " 'day unyummi food wagon': 3099484,\n",
       " 'unyummi food wagon went': 14936219,\n",
       " 'food wagon went realli': 4574176,\n",
       " 'wagon went realli weather': 15179861,\n",
       " 'went realli weather perfect': 15639101,\n",
       " 'realli weather perfect ride': 11457731,\n",
       " 'weather perfect ride home': 15514316,\n",
       " 'perfect ride home tomorrow': 10490747,\n",
       " 'ride home tomorrow bring': 11669146,\n",
       " 'day unyummi food wagon went': 3099485,\n",
       " 'unyummi food wagon went realli': 14936220,\n",
       " 'food wagon went realli weather': 4574177,\n",
       " 'wagon went realli weather perfect': 15179862,\n",
       " 'went realli weather perfect ride': 15639102,\n",
       " 'realli weather perfect ride home': 11457732,\n",
       " 'weather perfect ride home tomorrow': 15514317,\n",
       " 'perfect ride home tomorrow bring': 10490748,\n",
       " 'got': 5248066,\n",
       " 'holist': 6143050,\n",
       " 'hoodi': 6223012,\n",
       " 'attempt': 710058,\n",
       " 'cheer': 2106766,\n",
       " 'pretti': 10922640,\n",
       " 'comfi': 2474545,\n",
       " 'got new': 5305201,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trunc = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "x_train_trunc_vectorizer=count_trunc.fit_transform(x_train_trunc)\n",
    "x_test_trunc_vectorizer=count_trunc.transform(x_test_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**XGBoost**](##XGBoost)\n",
    "* [**Keras**](##Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm used for classification and regression tasks. It combines multiple weak models, typically decision trees, to create a stronger and more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the intention to employ a neural network for text classification, an XGBoost model has been trained to evaluate its performance and compare it with the results of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with vectorizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10688  9219]\n",
      " [ 2670 17423]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.54      0.64     19907\n",
      "           1       0.65      0.87      0.75     20093\n",
      "\n",
      "    accuracy                           0.70     40000\n",
      "   macro avg       0.73      0.70      0.69     40000\n",
      "weighted avg       0.73      0.70      0.69     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model_vectorizer = xgb_model.fit(x_train_trunc_vectorizer, y_train_trunc)\n",
    "xgb_predictions_vectorizer=xgb_model_vectorizer.predict(x_test_trunc_vectorizer)\n",
    "print(confusion_matrix(y_test_trunc,xgb_predictions_vectorizer))\n",
    "print (classification_report(y_test_trunc, xgb_predictions_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is being processed using the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 300\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train_trunc)\n",
    "sequences = tokenizer.texts_to_sequences(x_train_trunc)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Embedding Layer:** This layer converts input words into dense vectors of fixed length. The vocabulary size is set to 50,000 words, and each word is represented as a 100-dimensional vector. The input size of this layer is a maximum length of 300 words.\n",
    "\n",
    "* **Spatial Dropout1D Layer:** This layer applies dropout to prevent overfitting. It randomly drops out entire channels (feature maps) instead of individual neurons. In this case, 20% of the outputs from the Embedding layer are randomly set to 0.\n",
    "\n",
    "* **LSTM Layer:** This layer utilizes LSTM (Long Short-Term Memory) units to model the sequence of words in the text. Each LSTM unit has 100 memory cells and can capture long-term patterns in sequential data. The LSTM layer also applies dropout with a rate of 20% on the recurrent connections to prevent overfitting.\n",
    "\n",
    "* **Dense Layer:** This is the output layer of the model, consisting of a single neuron with a sigmoid activation function. It produces an output between 0 and 1, representing the probability of the text instance belonging to a particular class (e.g., positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 15:09:57.662229: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-03-10 15:09:57.662270: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-03-10 15:09:57.662285: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-03-10 15:09:57.662351: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-10 15:09:57.662374: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 300, 100)          0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5080501 (19.38 MB)\n",
      "Trainable params: 5080501 (19.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EarlyStopping:**\n",
    "* monitor='val_accuracy': It monitors the validation accuracy during training.\n",
    "* mode='max': It maximizes the monitored metric (validation accuracy in this case).\n",
    "* patience=5: It specifies the number of epochs to wait before stopping the training process if the monitored metric doesn't improve.\n",
    "\n",
    "<br>\n",
    "\n",
    "**ModelCheckpoint:**\n",
    "* filepath='./keras': It specifies the path and filename to save the model weights.\n",
    "* save_weights_only=True: It indicates that only the weights of the best model will be saved, not the entire model.\n",
    "* monitor='val_accuracy': It monitors the validation accuracy during training.\n",
    "* mode='max': It maximizes the monitored metric (validation accuracy in this case).\n",
    "* save_best_only=True: It saves only the weights of the best model based on the monitored metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    mode='max',\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "checkpoint= ModelCheckpoint(\n",
    "    filepath='./keras',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 15:10:03.855780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.7108  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 17:48:27.623543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 9606s 128s/step - loss: 0.5730 - accuracy: 0.7108 - val_loss: 0.5139 - val_accuracy: 0.7536\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 9555s 127s/step - loss: 0.4741 - accuracy: 0.7796 - val_loss: 0.4987 - val_accuracy: 0.7595\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 9629s 128s/step - loss: 0.4356 - accuracy: 0.7999 - val_loss: 0.4976 - val_accuracy: 0.7581\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 9619s 128s/step - loss: 0.4104 - accuracy: 0.8160 - val_loss: 0.5047 - val_accuracy: 0.7523\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 9569s 128s/step - loss: 0.3887 - accuracy: 0.8291 - val_loss: 0.5118 - val_accuracy: 0.7531\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 9593s 128s/step - loss: 0.3680 - accuracy: 0.8381 - val_loss: 0.5158 - val_accuracy: 0.7509\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 9652s 129s/step - loss: 0.3492 - accuracy: 0.8490 - val_loss: 0.5277 - val_accuracy: 0.7459\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(sequences_matrix,y_train_trunc,batch_size=1024,epochs=10,\n",
    "            validation_split=0.2,callbacks=[stop,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 36799s 3s/step - loss: 0.5163 - accuracy: 0.7539\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 22:57:20.583270: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9875/12500 [======================>.......] - ETA: 2:34:31"
     ]
    }
   ],
   "source": [
    "lstm_prediction=model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlstm_prediction\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m      4\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "res=[]\n",
    "for prediction in lstm_prediction:\n",
    "    if prediction[0]<0.5:\n",
    "        res.append(0)\n",
    "    else:\n",
    "        res.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3125813826.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(confusion_matrix(y_test,res))Ç\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,res))Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(tokenizer, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model=keras.models.load_model(\"./sentiment_model.h5\")\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'I hate you idiot'\n",
    "\n",
    "test=[clean(test)]\n",
    "print(test)\n",
    "seq = load_tokenizer.texts_to_sequences(test)\n",
    "padded = sequence.pad_sequences(seq, maxlen=300)\n",
    "print(seq)\n",
    "pred = load_model.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if pred<0.3:\n",
    "    print(\"positive\")\n",
    "else:\n",
    "    print(\"negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
